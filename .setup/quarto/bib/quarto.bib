@Article{Almeida-2005,
  author = {David M. Almeida},
  date = {2005-04},
  journaltitle = {Current Directions in Psychological Science},
  title = {Resilience and vulnerability to daily stressors assessed via diary methods},
  doi = {10.1111/j.0963-7214.2005.00336.x},
  issn = {1467-8721},
  number = {2},
  pages = {64--68},
  volume = {14},
  abstract = {Stressors encountered in daily life, such as family arguments or work deadlines, may play an important role in individual health and well-being. This article presents a framework for understanding how characteristics of individuals and their environments limit or increase exposure and reactivity to daily stressors. Research on daily stressors has benefited from diary methods that obtain repeated measurements from individuals during their daily lives. These methods improve ecological validity, reduce memory distortions, and permit the assessment of within-person processes. Findings from the National Study of Daily Experiences, which used a telephone-diary design, highlight how people's age, gender, and education and the presence or absence of chronic stressors in their lives predict their exposure and reactivity to daily stressors. Finally, future directions for research designs that combine laboratory-based assessment of stress physiology with daily-diary methods are discussed.},
  publisher = {SAGE Publications},
}

@Article{Andrews-2000,
  author = {Donald W. K. Andrews},
  date = {2000-03},
  journaltitle = {Econometrica},
  title = {Inconsistency of the bootstrap	when a parameter is on the boundary of the parameter space},
  doi = {10.1111/1468-0262.00114},
  number = {2},
  pages = {399--405},
  volume = {68},
  publisher = {The Econometric Society},
}

@Article{Arnett-2005,
  author = {Jeffrey Jensen Arnett},
  date = {2005-04},
  journaltitle = {Journal of Drug Issues},
  title = {The developmental context of substance use in emerging adulthood},
  doi = {10.1177/002204260503500202},
  issn = {1945-1369},
  number = {2},
  pages = {235--254},
  volume = {35},
  abstract = {The theory of emerging adulthood has been proposed as a way of conceptualizing the developmental characteristics of young people between the ages of 18 and 25. Here, the theory is applied to explaining the high rates of substance use in this age group. Specifically, five developmentally distinctive features of emerging adulthood are proposed: the age of identity explorations, the age of instability, the age of self-focus, the age of feeling in-between, and the age of possibilities. Then, each of these features is applied to an explanation of drug use in emerging adulthood.},
  publisher = {SAGE Publications},
}

@Article{Baker-Piper-McCarthy-etal-2004,
  author = {Timothy B. Baker and Megan E. Piper and Danielle E. McCarthy and Matthew R. Majeskie and Michael C. Fiore},
  date = {2004},
  journaltitle = {Psychological Review},
  title = {Addiction motivation reformulated: An affective processing model of negative reinforcement},
  doi = {10.1037/0033-295x.111.1.33},
  issn = {0033-295X},
  number = {1},
  pages = {33--51},
  volume = {111},
  abstract = {This article offers a reformulation of the negative reinforcement model of drug addiction and proposes that the escape and avoidance of negative affect is the prepotent motive for addictive drug use. The authors posit that negative affect is the motivational core of the withdrawal syndrome and argue that, through repeated cycles of drug use and withdrawal, addicted organisms learn to detect interoceptive cues of negative affect preconsciously. Thus, the motivational basis of much drug use is opaque and tends not to reflect cognitive control. When either stressors or abstinence causes negative affect to grow and enter consciousness, increasing negative affect biases information processing in ways that promote renewed drug administration. After explicating their model, the authors address previous critiques of negative reinforcement models in light of their reformulation and review predictions generated by their model.},
  publisher = {American Psychological Association (APA)},
}

@Article{Bauer-Curran-2005,
  author = {Daniel J. Bauer and Patrick J. Curran},
  date = {2005-07},
  journaltitle = {Multivariate Behavioral Research},
  title = {Probing interactions in fixed and multilevel regression: Inferential and graphical techniques},
  doi = {10.1207/s15327906mbr4003_5},
  issn = {1532-7906},
  number = {3},
  pages = {373--400},
  volume = {40},
  abstract = {Many important research hypotheses concern conditional relations in which the effect of one predictor varies with the value of another. Such relations are commonly evaluated as multiplicative interactions and can be tested in both fixed- and random-effects regression. Often, these interactive effects must be further probed to fully explicate the nature of the conditional relation. The most common method for probing interactions is to test simple slopes at specific levels of the predictors. A more general method is the Johnson-Neyman (J-N) technique. This technique is not widely used, however, because it is currently limited to categorical by continuous interactions in fixed-effects regression and has yet to be extended to the broader class of random-effects regression models. The goal of our article is to generalize the J-N technique to allow for tests of a variety of interactions that arise in both fixed- and random-effects regression. We review existing methods for probing interactions, explicate the analytic expressions needed to expand these tests to a wider set of conditions, and demonstrate the advantages of the J-N technique relative to simple slopes with three empirical examples.},
  publisher = {Informa UK Limited},
}

@Article{Bauer-Preacher-Gil-2006,
  author = {Daniel J. Bauer and Kristopher J. Preacher and Karen M. Gil},
  date = {2006},
  journaltitle = {Psychological Methods},
  title = {Conceptualizing and testing random indirect effects and moderated mediation in multilevel models: New procedures and recommendations},
  doi = {10.1037/1082-989x.11.2.142},
  number = {2},
  pages = {142--163},
  volume = {11},
  abstracts = {The authors propose new procedures for evaluating direct, indirect, and total effects in multilevel models when all relevant variables are measured at Level 1 and all effects are random. Formulas are provided for the mean and variance of the indirect and total effects and for the sampling variances of the average indirect and total effects. Simulations show that the estimates are unbiased under most conditions. Confidence intervals based on a normal approximation or a simulated sampling distribution perform well when the random effects are normally distributed but less so when they are nonnormally distributed. These methods are further developed to address hypotheses of moderated mediation in the multilevel context. An example demonstrates the feasibility and usefulness of the proposed methods.},
  publisher = {American Psychological Association ({APA})},
  keywords = {multilevel model, hierarchical linear model, indirect effect, mediation, moderated mediation},
  annotation = {mediation, mediation-multilevel},
}

@Article{Bentler-2007,
  author = {Peter M. Bentler},
  date = {2007},
  journaltitle = {American Psychologist},
  title = {Can scientifically useful hypotheses be tested with correlations?},
  doi = {10.1037/0003-066x.62.8.772},
  issn = {0003-066X},
  number = {8},
  pages = {772--782},
  volume = {62},
  abstract = {Historically, interesting psychological theories have been phrased in terms of correlation coefficients, which are standardized covariances, and various statistics derived from them. Methodological practice over the last 40 years, however, has suggested it is necessary to transform such theories into hypotheses on covariances and statistics derived from them. This complication turns out to be unnecessary, because the methodology now exists to test hypotheses on latent structures of correlations directly. Two examples are given. Limitations of correlation structures are also noted.},
  publisher = {American Psychological Association (APA)},
}

@Article{Boker-2002,
  author = {Steven M. Boker},
  date = {2002-07},
  journaltitle = {Multivariate Behavioral Research},
  title = {Consequences of continuity: The hunt for intrinsic properties within parameters of dynamics in psychological processes},
  doi = {10.1207/s15327906mbr3703_5},
  issn = {1532-7906},
  number = {3},
  pages = {405--422},
  volume = {37},
  abstract = {Notes that over 300 yrs ago Sir Isaac Newton wrote of a simple set of relations that could be used to predict the motions of objects relative to one another. The main advantage of this insight was that the relationship between the movements of the planets and stars could be predicted much more simply than with the accurate, but cumbersome Ptolemaic calculations. But perhaps the most important consequence of the acceptance of Newton's insight was that intrinsic properties such as mass could be distinguished from measurements such as weight. A similar revolution in thinking appears to be underway in the behavioral sciences. It is likely that intensive longitudinal measurement coupled with dynamical systems analyses will lead to simplified but powerful models of the evolution of psychological processes. In this case, it is reasonable to expect that a set of intrinsic psychological properties may be able to be extracted from the parameters of successful dynamical systems models. The purpose of this article is to issue an invitation to the hunt, to provide a tentative map as to where the game might likely be found, and blow a call on the hunting horn.},
  publisher = {Informa UK Limited},
}

@Article{Bolger-Davis-Rafaeli-2003,
  author = {Niall Bolger and Angelina Davis and Eshkol Rafaeli},
  date = {2003-02},
  journaltitle = {Annual Review of Psychology},
  title = {Diary methods: Capturing life as it is lived},
  doi = {10.1146/annurev.psych.54.101601.145030},
  issn = {1545-2085},
  number = {1},
  pages = {579--616},
  volume = {54},
  abstract = {In diary studies, people provide frequent reports on the events and experiences of their daily lives. These reports capture the particulars of experience in a way that is not possible using traditional designs. We review the types of research questions that diary methods are best equipped to answer, the main designs that can be used, current technology for obtaining diary reports, and appropriate data analysis strategies. Major recent developments include the use of electronic forms of data collection and multilevel models in data analysis. We identify several areas of research opportunities: 1. in technology, combining electronic diary reports with collateral measures such as ambulatory heart rate; 2. in measurement, switching from measures based on between-person differences to those based on within-person changes; and 3. in research questions, using diaries to (a) explain why people differ in variability rather than mean level, (b) study change processes during major events and transitions, and (c) study interpersonal processes using dyadic and group diary methods.},
  publisher = {Annual Reviews},
  keywords = {experience sampling method, longitudinal designs, electronic data collection, self-report measures, multilevel models, diary},
}

@Article{Casella-2003,
  author = {George Casella},
  date = {2003-05},
  journaltitle = {Statistical Science},
  title = {Introduction to the silver anniversary of the bootstrap},
  doi = {10.1214/ss/1063994967},
  number = {2},
  volume = {18},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Efron-2003,
  author = {Bradley Efron},
  date = {2003-05},
  journaltitle = {Statistical Science},
  title = {Second thoughts on the bootstrap},
  doi = {10.1214/ss/1063994968},
  number = {2},
  volume = {18},
  abstract = {This brief review article is appearing in the issue of Statistical Science that marks the 25th anniversary of the bootstrap. It concerns some of the theoretical and methodological aspects of the bootstrap and how they might influence future work in statistics.},
  publisher = {Institute of Mathematical Statistics},
  keywords = {ABC method, BCA, bootstrap confidence intervals, objective Bayes, plug-in principle},
}

@Article{Davison-Hinkley-Young-2003,
  author = {Anthony Christopher Davison and David Victor Hinkley and George Alastair Young},
  date = {2003-05},
  journaltitle = {Statistical Science},
  title = {Recent developments in bootstrap methodology},
  doi = {10.1214/ss/1063994969},
  number = {2},
  volume = {18},
  abstract = {Ever since its introduction, the bootstrap has provided both a powerful set of solutions for practical statisticians, and a rich source of theoretical and methodological problems for statistics. In this article, some recent developments in bootstrap methodology are reviewed and discussed. After a brief introduction to the bootstrap, we consider the following topics at varying levels of detail: the use of bootstrapping for highly accurate parametric inference; theoretical properties of nonparametric bootstrapping with unequal probabilities; subsampling and the $m$ out of $n$ bootstrap; bootstrap failures and remedies for superefficient estimators; recent topics in significance testing; bootstrap improvements of unstable classifiers and resampling for dependent data. The treatment is telegraphic rather than exhaustive.},
  publisher = {Institute of Mathematical Statistics},
  keywords = {bagging, bootstrap, conditional inference, empirical strength probability, parametric bootstrap, subsampling, superefficient estimator, tilted distribution, time series, weighted bootstrap},
}

@Article{Hall-2003,
  author = {Peter Hall},
  date = {2003-05},
  journaltitle = {Statistical Science},
  title = {A short prehistory of the bootstrap},
  doi = {10.1214/ss/1063994970},
  number = {2},
  volume = {18},
  abstract = {The contemporary development of bootstrap methods, from the time of Efron's early articles to the present day, is well documented and widely appreciated. Likewise, the relationship of bootstrap techniques to certain early work on permutation testing, the jackknife and cross-validation is well understood. Less known, however, are the connections of the bootstrap to research on survey sampling for spatial data in the first half of the last century or to work from the 1940s to the 1970s on subsampling and resampling. In a selective way, some of these early linkages will be explored, giving emphasis to developments with which the statistics community tends to be less familiar. Particular attention will be paid to the work of P. C. Mahalanobis,	whose development in the 1930s and 1940s of moving-block sampling methods for spatial data has a range of interesting features, and to contributions of other scientists who, during the next 40 years, developed half-sampling, subsampling and resampling methods.},
  publisher = {Institute of Mathematical Statistics},
  keywords = {block bootstrap, computer-intensive statistics, confidence interval, half-sample, Monte Carlo, moving block, permutation test, resample, resampling, sample survey, statistical experimentation, sub-sample},
}

@Article{Boos-2003,
  author = {Dennis D. Boos},
  date = {2003-05},
  journaltitle = {Statistical Science},
  title = {Introduction to the bootstrap world},
  doi = {10.1214/ss/1063994971},
  number = {2},
  volume = {18},
  abstract = {The bootstrap has made a fundamental impact on how we carry out statistical inference in problems without analytic solutions. This fact is illustrated with examples and comments that emphasize the parametric bootstrap and hypothesis testing.},
  publisher = {Institute of Mathematical Statistics},
  keywords = {confidence intervals, hypothesis testing, resamples, resampling, statistical inference},
}

@Article{Beran-2003,
  author = {Rudolf Beran},
  date = {2003-05},
  journaltitle = {Statistical Science},
  title = {The impact of the bootstrap on statistical algorithms and theory},
  doi = {10.1214/ss/1063994972},
  number = {2},
  volume = {18},
  abstract = {Bootstrap ideas yield remarkably effective algorithms for realizing certain programs in statistics. These include the construction of (possibly simultaneous) confidences sets and tests in classical models for which exact or asymptotic distribution theory is intractable. Success of the bootstrap, in the sense of doing what is expected under a probability model for data, is not universal. Modifications to Efron's definition of the bootstrap are needed to make the idea work for modern procedures that are not classically regular.},
  publisher = {Institute of Mathematical Statistics},
  keywords = {confidence sets, convolution theorem, double bootstrap, error in coverage probability, local asymptotic equivariance, simultaneous confidence sets},
}

@Article{Lele-2003,
  author = {Subhash R. Lele},
  date = {2003-05},
  journaltitle = {Statistical Science},
  title = {Impact of bootstrap on the estimating functions},
  doi = {10.1214/ss/1063994973},
  number = {2},
  volume = {18},
  abstract = {Estimating functions form an attractive statistical methodology because of their dependence on only a few features of the underlying probabilistic structure. They also put a premium on developing methods that obtain model-robust confidence intervals. Bootstrap and jackknife ideas can be fruitfully used toward this purpose. Another important area in which bootstrap has proved its use is in the context of detecting the problem of multiple roots and searching for the consistent root of an estimating function. In this article, I review, compare and contrast various approaches for bootstrapping estimating functions.},
  publisher = {Institute of Mathematical Statistics},
  keywords = {model-robust confidence intervals, multiple roots, stochastic processes, Wu's wild bootstrap},
}

@Article{Shao-2003,
  author = {Jun Shao},
  date = {2003-05},
  journaltitle = {Statistical Science},
  title = {Impact of the bootstrap on sample surveys},
  doi = {10.1214/ss/1063994974},
  number = {2},
  volume = {18},
  abstract = {This article discusses the impact of the bootstrap on sample surveys and introduces some of the main developments of the bootstrap methodology for sample surveys in the last twenty five years.},
  publisher = {Institute of Mathematical Statistics},
  keywords = {easy implementation, imputation, robustness, stratification, variance estimation, without replacement sampling},
}

@Article{Lahiri-2003,
  author = {Partha Lahiri},
  date = {2003-05},
  journaltitle = {Statistical Science},
  title = {On the impact of bootstrap in survey sampling and small-area estimation},
  doi = {10.1214/ss/1063994975},
  number = {2},
  volume = {18},
  abstract = {Development of valid bootstrap procedures has been a challenging problem for survey samplers for the last two decades. This is due to the fact that in surveys we constantly face various complex issues such as complex correlation structure induced by the survey design, weighting, imputation, small-area estimation, among others. In this paper, we critically review various bootstrap methods developed to deal with these challenging issues. We discuss two applications where the bootstrap has been found to be effective.},
  publisher = {Institute of Mathematical Statistics},
  keywords = {imputation, resampling, small-area estimation, survey weights},
}

@Article{Horowitz-2003,
  author = {Joel L. Horowitz},
  date = {2003-05},
  journaltitle = {Statistical Science},
  title = {The bootstrap in econometrics},
  doi = {10.1214/ss/1063994976},
  number = {2},
  volume = {18},
  abstract = {This paper presents examples of problems in estimation and hypothesis testing that demonstrate the use and performance of the bootstrap in econometric settings. The examples are illustrated with two empirical applications. The paper concludes with a discussion of topics on which further research is needed.},
  publisher = {Institute of Mathematical Statistics},
  keywords = {asymptotic distribution, asymptotic refinement, hypothesis test},
}

@Article{Politis-2003,
  author = {Dimitris N. Politis},
  date = {2003-05},
  journaltitle = {Statistical Science},
  title = {The impact of bootstrap methods on time series analysis},
  doi = {10.1214/ss/1063994977},
  number = {2},
  volume = {18},
  abstract = {Sparked by Efron's seminal paper, the decade of the 1980s was a period of active research on bootstrap methods for independent data--mainly i.i.d. or regression set-ups. By contrast, in the 1990s much research was directed towards resampling dependent data, for example, time series and random fields. Consequently, the availability of valid nonparametric inference procedures based on resampling and/or subsampling has freed practitioners from the necessity of resorting to simplifying assumptions such as normality or linearity that may be misleading.},
  publisher = {Institute of Mathematical Statistics},
  keywords = {block bootstrap, confidence intervals, large sample inference, linear models, nonparametric estimation, resampling, subsampling},
}

@Article{Ernst-Hutson-2003,
  author = {Michael D. Ernst and Alan D. Hutson},
  date = {2003-05},
  journaltitle = {Statistical Science},
  title = {Utilizing a quantile function approach to obtain exact bootstrap solutions},
  doi = {10.1214/ss/1063994978},
  number = {2},
  volume = {18},
  abstract = {The popularity of the bootstrap is due in part to its wide applicability and the ease of implementing resampling procedures on modern computers. But careful reading of Efron (1979) will show that at its heart, the bootstrap is a ``plug-in'' procedure that involves calculating a functional $\theta \left( \hat{F} \right)$ from an estimate of the c.d.f. $F$. Resampling becomes invaluable when, as is often the case, $\theta \left( \hat{F} \right)$ cannot be calculated explicitly. We discuss some situations where working with the sample quantile function, $\hat{Q}$, rather than $\hat{F}$, can lead to explicit (exact) solutions to $\theta \left( \hat{F} \right)$.},
  publisher = {Institute of Mathematical Statistics},
  keywords = {censored data, confidence band, L-estimator, Monte Carlo, order statistics},
}

@Article{Holmes-2003a,
  author = {Susan Holmes},
  date = {2003-05},
  journaltitle = {Statistical Science},
  title = {Bootstrapping phylogenetic trees: Theory and methods},
  doi = {10.1214/ss/1063994979},
  number = {2},
  volume = {18},
  abstract = {This is a survey of the use of the bootstrap in the area of systematic and evolutionary biology. I present the current usage by biologists of the bootstrap as a tool both for making inferences and for evaluating robustness, and propose a framework for thinking about these problems in terms of mathematical statistics.},
  publisher = {Institute of Mathematical Statistics},
  keywords = {bootstrap, confidence regions, nonpositive curvature, phylogenetic trees},
}

@Article{Soltis-Soltis-2003,
  author = {Pamela S. Soltis and Douglas E. Soltis},
  date = {2003-05},
  journaltitle = {Statistical Science},
  title = {Applying the Bootstrap in Phylogeny Reconstruction},
  doi = {10.1214/ss/1063994980},
  number = {2},
  volume = {18},
  abstract = {With the increasing emphasis in biology on reconstruction of phylogenetic trees, questions have arisen as to how confident one should be in a given phylogenetic tree and how support for phylogenetic trees should be measured. Felsenstein suggested that bootstrapping be applied across characters of a taxon-by-character data matrix to produce replicate ``bootstrap data sets,'' each of which is then analyzed phylogenetically, with a consensus tree constructed to summarize the results of all replicates. The proportion of trees/replicates in which a grouping is recovered is presented as a measure of support for that group. Bootstrapping has become a common feature of phylogenetic analysis. However, the interpretation of bootstrap values remains open to discussion, and phylogeneticists have used these values in multiple ways. The usefulness of phylogenetic bootstrapping is potentially limited by a number of features, such as the size of the data matrix and the underlying assumptions of the phylogeny reconstruction program. Recent studies have explored the application of bootstrapping to large data sets and the relative performance of bootstrapping and jackknifing.},
  publisher = {Institute of Mathematical Statistics},
  keywords = {bootstrap, jackknife, phylogeny, support},
}

@Article{Holmes-2003b,
  author = {Susan Holmes},
  date = {2003-05},
  journaltitle = {Statistical Science},
  title = {{Bradley Efron}: A conversation with good friends},
  doi = {10.1214/ss/1063994981},
  number = {2},
  volume = {18},
  abstract = {Bradley Efron is Professor of Statistics and Biostatistics at Stanford University. He works on a combination of theoretical and applied topics, including empirical Bayes, survival analysis, exponential families, bootstrap and jackknife methods and confidence intervals. Most of his applied work has originated in biomedical consulting projects at the Stanford Medical School, mixed in with a few papers concerning astronomy and physics. Even his theoretical papers usually begin with specific applied problems. All three of the interviewers here have been close scientific collaborators.
  Brad was born in St. Paul, Minnestora, May 1938, to Esther and Miles Efron, Jewish-Russian immigrants. A Merit Scholarship, in the program's inaugural year, brought him to Caltech, graduating in Mathematics in 1960. He arrived at Stanford that Fall, eventually gaining his Ph.D., under the direction of Rupert Miller and Herb Solomon, in the Statistics Department, whose faculty also included Charles Stein, Herman Chernoff, Manny Parzen, Lincoln Moses and Ingram Olkin. Brad has lived at Stanford since 1960, with sabbaticals at Harvard, Imperial College and Berkeley. He has held several administrative positions in the university:	Chair of Statistics, Associate Dean of Science, Chairman of the University Advisory Board and Chair of the Faculty Senate. He is currently Chair of the Undergraduate Program in Applied Mathematics.
  Honors include doctorates from Chicago, Madrid and Oslo, a MacArthur Prize Fellowship, membership in the National Academy of Sciences and the American Academy of Arts and Sciences, fellowship in the IMS and ASA, the Wilks Medal, Parzen Prize, the newly inaugurated Rao Prize and the outstanding statistician award from the Chicago ASA chapter. He has been the Rietz, Wald, and Fisher lecturers and holds the Max H. Stein endowed chair as Professor of Humanities and Sciences at Stanford. Professional service includes Theory and Methods Editor of JASA and President of the IMS. Currently he is President-Elect of the American Statistical Association, becoming President in 2004.},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Buehlmann-2002,
  author = {Peter B{\"u}hlmann},
  date = {2002-05},
  journaltitle = {Statistical Science},
  title = {Bootstraps for time series},
  doi = {10.1214/ss/1023798998},
  issn = {0883-4237},
  number = {1},
  volume = {17},
  abstract = {We review and compare block, sieve and local bootstraps for time series and thereby illuminate theoretical aspects of the procedures as well as their performance on finite-sample data. Our view is selective with the intention of providing a new and fair picture of some particular aspects of bootstrapping time series.
The generality of the block bootstrap is contrasted with sieve bootstraps. We discuss implementational advantages and disadvantages. We argue that two types of sieve often outperform the block method, each of them in its own important niche, namely linear and categorical processes. Local bootstraps, designed for nonparametric smoothing problems, are easy to use and implement but exhibit in some cases low performance.},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Casey-Jones-Hare-2008,
  author = {B.J. Casey and Rebecca M. Jones and Todd A. Hare},
  date = {2008-03},
  journaltitle = {Annals of the New York Academy of Sciences},
  title = {The adolescent brain},
  doi = {10.1196/annals.1440.010},
  issn = {1749-6632},
  number = {1},
  pages = {111--126},
  volume = {1124},
  abstract = {Adolescence is a developmental period characterized by suboptimal decisions and actions that are associated with an increased incidence of unintentional injuries, violence, substance abuse, unintended pregnancy, and sexually transmitted diseases. Traditional neurobiological and cognitive explanations for adolescent behavior have failed to account for the nonlinear changes in behavior observed during adolescence, relative to both childhood and adulthood. This review provides a biologically plausible model of the neural mechanisms underlying these nonlinear changes in behavior. We provide evidence from recent human brain imaging and animal studies that there is a heightened responsiveness to incentives and socioemotional contexts during this time, when impulse control is still relatively immature. These findings suggest differential development of bottom-up limbic systems, implicated in incentive and emotional processing, to top-down control systems during adolescence as compared to childhood and adulthood. This developmental pattern may be exacerbated in those adolescents prone to emotional reactivity, increasing the likelihood of poor outcomes.},
  publisher = {Wiley},
}

@Article{Cheong-MacKinnon-Khoo-2003,
  author = {JeeWon Cheong and David P. MacKinnon and Siek Toon Khoo},
  date = {2003-04},
  journaltitle = {Structural Equation Modeling: A Multidisciplinary Journal},
  title = {Investigation of mediational processes using parallel process latent growth curve modeling},
  doi = {10.1207/s15328007sem1002_5},
  number = {2},
  pages = {238--262},
  volume = {10},
  abstract = {This study investigated a method to evaluate mediational processes using latent growth curve modeling. The mediator and the outcome measured across multiple time points were viewed as 2 separate parallel processes. The mediational process was defined as the independent variable influencing the growth of the mediator, which, in turn, affected the growth of the outcome. To illustrate modeling procedures, empirical data from a longitudinal drug prevention program, Adolescents Training and Learning to Avoid Steroids, were used. The program effects on the growth of the mediator and the growth of the outcome were examined first in a 2-group structural equation model. The mediational process was then modeled and tested in a parallel process latent growth curve model by relating the prevention program condition, the growth rate factor of the mediator, and the growth rate factor of the outcome.},
  publisher = {Informa {UK} Limited},
}

@Article{Cheung-2007,
  author = {Mike W.-L. Cheung},
  date = {2007-05},
  journaltitle = {Structural Equation Modeling: A Multidisciplinary Journal},
  title = {Comparison of approaches to constructing confidence intervals for mediating effects using structural equation models},
  doi = {10.1080/10705510709336745},
  number = {2},
  pages = {227--246},
  volume = {14},
  abstract = {Mediators are variables that explain the association between an independent variable and a dependent variable. Structural equation modeling (SEM) is widely used to test models with mediating effects. This article illustrates how to construct confidence intervals (CIs) of the mediating effects for a variety of models in SEM. Specifically, mediating models with 1 mediator, 2 intermediate mediators, 2 specific mediators, and 1 mediator in 2 independent groups are illustrated. By using phantom variables (Rindskopf, 1984), a Wald CI, percentile bootstrap CI, bias-corrected bootstrap CI, and a likelihood-based CI on the mediating effect are easily constructed with some existing SEM packages, such as LISREL, Mplus, and Mx. Monte Carlo simulation studies are used to compare the coverage probabilities of these CIs. The results show that the coverage probabilities of these CIs are comparable when the mediating effect is large or when the sample size is large. However, when the mediating effect and the sample size are both small, the bootstrap CI and likelihood-based CI are preferred over the Wald CI. Extensions of this SEM approach for future research are discussed.},
  publisher = {Informa {UK} Limited},
  keywords = {mediation, bootstrapping},
  annotation = {mediation, mediation-delta, mediation-likelihood, mediation-bootstrap},
}

@Article{Cheung-2008,
  author = {Mike W.-L. Cheung},
  date = {2008},
  journaltitle = {Psychological Methods},
  title = {A model for integrating fixed-, random-, and mixed-effects meta-analyses into structural equation modeling},
  doi = {10.1037/a0013163},
  issn = {1082-989X},
  number = {3},
  pages = {182--202},
  volume = {13},
  abstract = {Meta-analysis and structural equation modeling (SEM) are two important statistical methods in the behavioral, social, and medical sciences. They are generally treated as two unrelated topics in the literature. The present article proposes a model to integrate fixed-, random-, and mixed-effects meta-analyses into the SEM framework. By applying an appropriate transformation on the data, studies in a meta-analysis can be analyzed as subjects in a structural equation model. This article also highlights some practical benefits of using the SEM approach to conduct a meta-analysis. Specifically, the SEM-based meta-analysis can be used to handle missing covariates, to quantify the heterogeneity of effect sizes, and to address the heterogeneity of effect sizes with mixture models. Examples are used to illustrate the equivalence between the conventional meta-analysis and the SEM-based meta-analysis. Future directions on and issues related to the SEM-based meta-analysis are discussed.},
  publisher = {American Psychological Association (APA)},
}

@Article{Cheung-2009a,
  author = {Mike W.-L. Cheung},
  date = {2009-05},
  journaltitle = {Behavior Research Methods},
  title = {Comparison of methods for constructing confidence intervals of standardized indirect effects},
  doi = {10.3758/brm.41.2.425},
  number = {2},
  pages = {425--438},
  volume = {41},
  abstract = {Mediation models are often used as a means to explain the psychological mechanisms between an independent and a dependent variable in the behavioral and social sciences. A major limitation of the unstandardized indirect effect calculated from raw scores is that it cannot be interpreted as an effect-size measure. In contrast, the standardized indirect effect calculated from standardized scores can be a good candidate as a measure of effect size because it is scale invariant. In the present article, 11 methods for constructing the confidence intervals (CIs) of the standardized indirect effects were evaluated via a computer simulation. These included six Wald CIs, three bootstrap CIs, one likelihood-based CI, and the PRODCLIN CI. The results consistently showed that the percentile bootstrap, the bias-corrected bootstrap, and the likelihood-based approaches had the best coverage probability. Mplus, LISREL, and Mx syntax were included to facilitate the use of these preferred methods in applied settings. Future issues on the use of the standardized indirect effects are discussed.},
  publisher = {Springer Science and Business Media {LLC}},
  keywords = {mediation analysis, coverage probability, structural equation modeling approach},
  annotation = {mediation, mediation-bootstrap, mediation-likelihood, mediation-delta, mediation-prodclin},
}

@Article{Cheung-2009b,
  author = {Mike W.-L. Cheung},
  date = {2009-04},
  journaltitle = {Structural Equation Modeling: A Multidisciplinary Journal},
  title = {Constructing approximate confidence intervals for parameters with structural equation models},
  doi = {10.1080/10705510902751291},
  number = {2},
  pages = {267--294},
  volume = {16},
  abstract = {Confidence intervals (CIs) for parameters are usually constructed based on the estimated standard errors. These are known as Wald CIs. This article argues that likelihood-based CIs (CIs based on likelihood ratio statistics) are often preferred to Wald CIs. It shows how the likelihood-based CIs and the Wald CIs for many statistics and psychometric indexes can be constructed with the use of phantom variables (Rindskopf, 1984) in some of the current structural equation modeling (SEM) packages. The procedures to form CIs for the differences in correlation coefficients, squared multiple correlations, indirect effects, coefficient alphas, and reliability estimates are illustrated. A simulation study on the Pearson correlation is used to demonstrate the advantages of the likelihood-based CI over the Wald CI. Issues arising from this SEM approach and extensions of this approach are discussed.},
  publisher = {Informa {UK} Limited},
  annotation = {mediation, mediation-likelihood},
}

@Article{Cheung-Lau-2007,
  author = {Gordon W. Cheung and Rebecca S. Lau},
  date = {2007-07},
  journaltitle = {Organizational Research Methods},
  title = {Testing mediation and suppression effects of latent variables},
  doi = {10.1177/1094428107300343},
  number = {2},
  pages = {296--325},
  volume = {11},
  abstract = {Because of the importance of mediation studies, researchers have been continuously searching for the best statistical test for mediation effect. The approaches that have been most commonly employed include those that use zero-order and partial correlation, hierarchical regression models, and structural equation modeling (SEM). This study extends MacKinnon and colleagues (MacKinnon, Lockwood, Hoffmann, West, \& Sheets, 2002; MacKinnon, Lockwood, \& Williams, 2004, MacKinnon, Warsi, \& Dwyer, 1995) works by conducting a simulation that examines the distribution of mediation and suppression effects of latent variables with SEM, and the properties of confidence intervals developed from eight different methods. Results show that SEM provides unbiased estimates of mediation and suppression effects, and that the bias-corrected bootstrap confidence intervals perform best in testing for mediation and suppression effects. Steps to implement the recommended procedures with Amos are presented.},
  publisher = {{SAGE} Publications},
  keywords = {mediating effects, suppression effects, structural equation modeling},
  annotation = {mediation, mediation-bootstrap},
}

@Article{Chow-Hamagani-Nesselroade-2007,
  author = {Sy-Miin Chow and Fumiaki Hamagani and John R. Nesselroade},
  date = {2007-12},
  journaltitle = {Psychology and Aging},
  title = {Age differences in dynamical emotion-cognition linkages},
  doi = {10.1037/0882-7974.22.4.765},
  issn = {0882-7974},
  number = {4},
  pages = {765--780},
  volume = {22},
  abstract = {The ability to maintain the separation between positive emotion and negative emotion in times of stress has been construed as a resilience mechanism. Emotional resiliency is particularly relevant in old age given concomitant declines in cognitive performance. In the present study, the authors examined the dynamical linkages among positive emotion, negative emotion, and cognition as individuals performed a complex cognitive task. Comparisons were made between younger (n = 63) and older (n = 52) age groups. Older adults manifested significant unidirectional coupling from negative emotion to cognitive performance; younger adults manifested significant unidirectional coupling from negative emotion to positive emotion and from cognitive performance to both positive and negative emotions. Implications for age differences in emotion regulatory strategies are discussed.},
  publisher = {American Psychological Association (APA)},
}

@Article{Cole-Martin-Steiger-2005,
  author = {David A. Cole and Nina C. Martin and James H. Steiger},
  date = {2005-03},
  journaltitle = {Psychological Methods},
  title = {Empirical and conceptual problems with longitudinal trait-state models: Introducing a trait-state-occasion model},
  doi = {10.1037/1082-989x.10.1.3},
  issn = {1082-989X},
  number = {1},
  pages = {3--20},
  volume = {10},
  abstract = {The latent trait-state-error model (TSE) and the latent state-trait model with autoregression (LST-AR) represent creative structural equation methods for examining the longitudinal structure of psychological constructs. Application of these models has been somewhat limited by empirical or conceptual problems. In the present study, Monte Carlo analysis revealed that TSE models tend to generate improper solutions when N is too small, when waves are too few, and when occasion factor stability is either too large or too small. Mathematical analysis of the LST-AR model revealed its limitation to constructs that become more highly auto-correlated over time. The trait-state-occasion model has fewer empirical problems than does the TSE model and is more broadly applicable than is the LST-AR model.},
  publisher = {American Psychological Association (APA)},
}

@Article{Cole-Maxwell-2003,
  author = {David A. Cole and Scott E. Maxwell},
  date = {2003-11},
  journaltitle = {Journal of Abnormal Psychology},
  title = {Testing mediational models with longitudinal data: Questions and tips in the use of structural equation modeling.},
  doi = {10.1037/0021-843x.112.4.558},
  number = {4},
  pages = {558--577},
  volume = {112},
  abstract = {R. M. Baron and D. A. Kenny (1986; see record 1987-13085-001) provided clarion conceptual and methodological guidelines for testing mediational models with cross-sectional data. Graduating from cross-sectional to longitudinal designs enables researchers to make more rigorous inferences about the causal relations implied by such models. In this transition, misconceptions and erroneous assumptions are the norm. First, we describe some of the questions that arise (and misconceptions that sometimes emerge) in longitudinal tests of mediational models. We also provide a collection of tips for structural equation modeling (SEM) of mediational processes. Finally, we suggest a series of 5 steps when using SEM to test mediational processes in longitudinal designs: testing the measurement model, testing for added components, testing for omitted paths, testing the stationarity assumption, and estimating the mediational effects.},
  publisher = {American Psychological Association ({APA})},
}

@Article{CribariNeto-2004,
  author = {Francisco Cribari-Neto},
  date = {2004-03},
  journaltitle = {Computational Statistics {\&} Data Analysis},
  title = {Asymptotic inference under heteroskedasticity of unknown form},
  doi = {10.1016/s0167-9473(02)00366-3},
  number = {2},
  pages = {215--233},
  volume = {45},
  abstract = {We focus on the finite-sample behavior of heteroskedasticity-consistent covariance matrix estimators and associated quasi-$t$ tests. The estimator most commonly used is that proposed by Halbert White. Its finite-sample behavior under both homoskedasticity and heteroskedasticity is analyzed using Monte Carlo methods. We also consider two other consistent estimators, namely: the HC3 estimator, which is an approximation to the jackknife estimator, and the weighted bootstrap estimator. Additionally, we evaluate the finite-sample behavior of two bootstrap quasi-$t$ tests: the test based on a single bootstrapping scheme and the test based on a double, nested bootstrapping scheme. The latter is very computer-intensive, but proves to work well in small samples. Finally, we propose a new estimator, which we call HC4; it is tailored to take into account the effect of leverage points in the design matrix on associated quasi-$t$ tests.},
  publisher = {Elsevier {BV}},
  annotation = {regression, regression-hc},
}

@Article{CribariNeto-daSilva-2010,
  author = {Francisco Cribari-Neto and Wilton Bernardino {da Silva}},
  date = {2010-11},
  journaltitle = {{AStA} Advances in Statistical Analysis},
  title = {A new heteroskedasticity-consistent covariance matrix estimator for the linear regression model},
  doi = {10.1007/s10182-010-0141-2},
  number = {2},
  pages = {129--146},
  volume = {95},
  abstract = {The assumption that all random errors in the linear regression model share the same variance (homoskedasticity) is often violated in practice. The ordinary least squares estimator of the vector of regression parameters remains unbiased, consistent and asymptotically normal under unequal error variances. Many practitioners then choose to base their inferences on such an estimator. The usual practice is to couple it with an asymptotically valid estimation of its covariance matrix, and then carry out hypothesis tests that are valid under heteroskedasticity of unknown form. We use numerical integration methods to compute the exact null distributions of some quasi-t test statistics, and propose a new covariance matrix estimator. The numerical results favor testing inference based on the estimator we propose.},
  publisher = {Springer Science and Business Media {LLC}},
  annotation = {regression, regression-hc},
}

@Article{CribariNeto-Souza-Vasconcellos-2007,
  author = {Francisco Cribari-Neto and Tatiene C. Souza and Klaus L. P. Vasconcellos},
  date = {2007-08},
  journaltitle = {Communications in Statistics - Theory and Methods},
  title = {Inference under heteroskedasticity and leveraged data},
  doi = {10.1080/03610920601126589},
  number = {10},
  pages = {1877--1888},
  volume = {36},
  abstract = {We evaluate the finite-sample behavior of different heteros-ke-das-ticity-consistent covariance matrix estimators, under both constant and unequal error variances. We consider the estimator proposed by Halbert White (HC0), and also its variants known as HC2, HC3, and HC4; the latter was recently proposed by Cribari-Neto (2004). We propose a new covariance matrix estimator: HC5. It is the first consistent estimator to explicitly take into account the effect that the maximal leverage has on the associated inference. Our numerical results show that quasi-$t$ inference based on HC5 is typically more reliable than inference based on other covariance matrix estimators.},
  publisher = {Informa {UK} Limited},
  annotation = {regression, regression-hc},
}

@Article{CribariNeto-Souza-Vasconcellos-2008,
  author = {Francisco Cribari-Neto and Tatiene C. Souza and Klaus L. P. Vasconcellos},
  date = {2008-09},
  journaltitle = {Communications in Statistics - Theory and Methods},
  title = {Errata: Inference under heteroskedasticity and leveraged data, {Communications in Statistics, Theory and Methods}, 36, 1877--1888, 2007},
  doi = {10.1080/03610920802109210},
  number = {20},
  pages = {3329--3330},
  volume = {37},
  publisher = {Informa {UK} Limited},
  annotation = {regression, regression-hc},
}

@Article{Fairchild-MacKinnon-Taborga-etal-2009,
  author = {Amanda J. Fairchild and David P. MacKinnon and Marcia P. Taborga and Aaron B. Taylor},
  date = {2009-05},
  journaltitle = {Behavior Research Methods},
  title = {$R^2$ effect-size measures for mediation analysis},
  doi = {10.3758/brm.41.2.486},
  issn = {1554-3528},
  number = {2},
  pages = {486--498},
  volume = {41},
  abstract = {$R^2$ effect-size measures are presented to assess variance accounted for in mediation models. The measures offer a means to evaluate both component paths and the overall mediated effect in mediation models. Statistical simulation results indicate acceptable bias across varying parameter and sample-size combinations. The measures are applied to a real-world example using data from a team-based health promotion program to improve the nutrition and exercise habits of firefighters. SAS and SPSS computer code are also provided for researchers to compute the measures in their own data.},
  publisher = {Springer Science and Business Media LLC},
}

@Article{Ferrer-McArdle-2003,
  author = {Emilio Ferrer and John McArdle},
  date = {2003-10},
  journaltitle = {Structural Equation Modeling: A Multidisciplinary Journal},
  title = {Alternative structural models for multivariate longitudinal data analysis},
  doi = {10.1207/s15328007sem1004_1},
  number = {4},
  pages = {493--524},
  volume = {10},
  abstract = {Structural equation models are presented as alternative models for examining longitudinal data. The models include (a) a cross-lagged regression model, (b) a factor model based on latent growth curves, and (c) a dynamic model based on latent difference scores. The illustrative data are on motivation and perceived competence of students during their first semester in high school. The 3 models yielded different results and such differences were discussed in terms of the conceptualization of change underlying each model. The last model was defended as the most reasonable for these data because it captured the dynamic interrelations between the examined constructs and, at the same time, identified potential growth in the variables.},
  publisher = {Informa {UK} Limited},
}

@Article{Flora-Curran-2004,
  author = {David B. Flora and Patrick J. Curran},
  date = {2004-12},
  journaltitle = {Psychological Methods},
  title = {An empirical evaluation of alternative methods of estimation for confirmatory factor analysis with ordinal data.},
  doi = {10.1037/1082-989x.9.4.466},
  issn = {1082-989X},
  number = {4},
  pages = {466--491},
  volume = {9},
  abstract = {Confirmatory factor analysis (CFA) is widely used for examining hypothesized relations among ordinal variables (e.g., Likert-type items). A theoretically appropriate method fits the CFA model to polychoric correlations using either weighted least squares (WLS) or robust WLS. Importantly, this approach assumes that a continuous, normal latent process determines each observed variable. The extent to which violations of this assumption undermine CFA estimation is not well-known. In this article, the authors empirically study this issue using a computer simulation study. The results suggest that estimation of polychoric correlations is robust to modest violations of underlying normality. Further, WLS performed adequately only at the largest sample size but led to substantial estimation difficulties with smaller samples. Finally, robust WLS performed well across all conditions.},
  publisher = {American Psychological Association (APA)},
}

@Article{Fritz-MacKinnon-2007,
  author = {Matthew S. Fritz and David P. MacKinnon},
  date = {2007-03},
  journaltitle = {Psychological Science},
  title = {Required sample size to detect the mediated effect},
  doi = {10.1111/j.1467-9280.2007.01882.x},
  number = {3},
  pages = {233--239},
  volume = {18},
  abstract = {Mediation models are widely used, and there are many tests of the mediated effect. One of the most common questions that researchers have when planning mediation studies is, ``How many subjects do I need to achieve adequate power when testing for mediation?'' This article presents the necessary sample sizes for six of the most common and the most recommended tests of mediation for various combinations of parameters, to provide a guide for researchers when designing studies or applying for grants.},
  publisher = {{SAGE} Publications},
  keywords = {bootstrap, collinearity, mediation analysis, power, tolerance},
  annotation = {mediation, mediation-power, mediation-causalsteps, mediation-joint, mediation-delta, mediation-prodclin, mediation-bootstrap},
}

@Article{Gatchel-Peng-Peters-etal-2007,
  author = {Robert J. Gatchel and Yuan Bo Peng and Madelon L. Peters and Perry N. Fuchs and Dennis C. Turk},
  date = {2007},
  journaltitle = {Psychological Bulletin},
  title = {The biopsychosocial approach to chronic pain: Scientific advances and future directions.},
  doi = {10.1037/0033-2909.133.4.581},
  issn = {0033-2909},
  number = {4},
  pages = {581--624},
  volume = {133},
  abstract = {The prevalence and cost of chronic pain is a major physical and mental health care problem in the United States today. As a result, there has been a recent explosion of research on chronic pain, with significant advances in better understanding its etiology, assessment, and treatment. The purpose of the present article is to provide a review of the most noteworthy developments in the field. The biopsychosocial model is now widely accepted as the most heuristic approach to chronic pain. With this model in mind, a review of the basic neuroscience processes of pain (the bio part of biopsychosocial), as well as the psychosocial factors, is presented. This spans research on how psychological and social factors can interact with brain processes to influence health and illness as well as on the development of new technologies, such as brain imaging, that provide new insights into brain-pain mechanisms.},
  publisher = {American Psychological Association (APA)},
}

@Article{Graham-Olchowski-Gilreath-2007,
  author = {John W. Graham and Allison E. Olchowski and Tamika D. Gilreath},
  date = {2007-06},
  journaltitle = {Prevention Science},
  title = {How many imputations are really needed? Some practical clarifications of multiple imputation theory},
  doi = {10.1007/s11121-007-0070-9},
  number = {3},
  pages = {206--213},
  volume = {8},
  abstract = {Multiple imputation (MI) and full information maximum likelihood (FIML) are the two most common approaches to missing data analysis. In theory, MI and FIML are equivalent when identical models are tested using the same variables, and when m, the number of imputations performed with MI, approaches infinity. However, it is important to know how many imputations are necessary before MI and FIML are sufficiently equivalent in ways that are important to prevention scientists. MI theory suggests that small values of m, even on the order of three to five imputations, yield excellent results. Previous guidelines for sufficient m are based on relative efficiency, which involves the fraction of missing information ($\gamma$) for the parameter being estimated, and m. In the present study, we used a Monte Carlo simulation to test MI models across several scenarios in which $\gamma$ and m were varied. Standard errors and p-values for the regression coefficient of interest varied as a function of m, but not at the same rate as relative efficiency. Most importantly, statistical power for small effect sizes diminished as m became smaller, and the rate of this power falloff was much greater than predicted by changes in relative efficiency. Based our findings, we recommend that researchers using MI should perform many more imputations than previously considered sufficient. These recommendations are based on $\gamma$, and take into consideration one's tolerance for a preventable power falloff (compared to FIML) due to using too few imputations.},
  publisher = {Springer Science and Business Media {LLC}},
  keywords = {multiple imputation, number of imputations, full information maximum likelihood, missing data, statistical power},
}

@Article{Grundy-Gondoli-BlodgettSalafia-2007,
  author = {Amber M. Grundy and Dawn M. Gondoli and Elizabeth H. {Blodgett Salafia}},
  date = {2007},
  journaltitle = {Journal of Family Psychology},
  title = {Marital conflict and preadolescent behavioral competence: Maternal knowledge as a longitudinal mediator},
  doi = {10.1037/0893-3200.21.4.675},
  issn = {0893-3200},
  number = {4},
  pages = {675--682},
  volume = {21},
  abstract = {The present study considered whether maternal knowledge mediated the relation between overt marital conflict and preadolescent behavioral competence. Four years of self-report data were collected from 133 mothers and their preadolescents, beginning when the preadolescents were in 4th grade. Marital conflict, maternal knowledge, and preadolescent behavioral competence were assessed at all 4 time points in order to apply a stringent methodology for assessing longitudinal mediating patterns. The results indicated that maternal knowledge mediated the relation between marital conflict and preadolescent behavioral competence. Thus, the present study identified one possible process through which marital conflict may affect preadolescent behavior.},
  publisher = {American Psychological Association (APA)},
}

@Article{Hamaker-Dolan-Molenaar-2005,
  author = {Ellen L. Hamaker and Conor V. Dolan and Peter C. M. Molenaar},
  date = {2005-04},
  journaltitle = {Multivariate Behavioral Research},
  title = {Statistical modeling of the individual: Rationale and application of multivariate stationary time series analysis},
  doi = {10.1207/s15327906mbr4002_3},
  issn = {1532-7906},
  number = {2},
  pages = {207--233},
  volume = {40},
  abstract = {Results obtained with interindividual techniques in a representative sample of a population are not necessarily generalizable to the individual members of this population. In this article the specific condition is presented that must be satisfied to generalize from the interindividual level to the intraindividual level. A way to investigate whether this condition is satisfied is by means of multivariate time series analysis. More generally, time series analysis can be used to investigate psychological processes situated within individuals. In this article we consider a well established class of multivariate stationary time series models that may be used to study the intraindividual covariance structure. We demonstrate the application of some of these models with an empirical example consisting of state measurements of behavior associated with the Five Factor Model of Personality. We illustrate how one can investigate whether individuals are similar with respect to their intraindividual structure of variation, and whether this structure is similar to the structure of interindividual variation.},
  publisher = {Informa UK Limited},
}

@Article{Hamaker-Nesselroade-Molenaar-2007,
  author = {Ellen L. Hamaker and John R. Nesselroade and Peter C.M. Molenaar},
  date = {2007-04},
  journaltitle = {Journal of Research in Personality},
  title = {The integrated trait-state model},
  doi = {10.1016/j.jrp.2006.04.003},
  issn = {0092-6566},
  number = {2},
  pages = {295--315},
  volume = {41},
  abstract = {It has been acknowledged that both trait and state contribute to psychological measurements. However, existing structural equation models for disentangling these sources of variability are based on assumptions that are not tenable in the light of empirical results. A new model is presented, termed the integrated trait-state (ITS) model, which both decomposes state and trait variance and allows one to test the assumptions that underly existing approaches. This is illustrated with an empirical example. The relationship between the ITS model and other analytic approaches as well as conceptual models of traits and states are discussed.},
  publisher = {Elsevier BV},
}

@Article{HatemiJ-2003,
  author = {Abdulnasser Hatemi-J},
  date = {2003-02},
  journaltitle = {Applied Economics Letters},
  title = {A new method to choose optimal lag order in stable and unstable {VAR} models},
  doi = {10.1080/1350485022000041050},
  number = {3},
  pages = {135--137},
  volume = {10},
  abstract = {A crucial aspect of empirical research based on the vector autoregressive (VAR) model is the choice of the lag order, since all inference in the VAR model is based on the chosen lag order. Here, a new information criterion is introduced for this purpose. The conducted Monte Carlo simulation experiments show that this new information criterion performs well in picking the true lag order in stable as well as unstable VAR models.},
  publisher = {Informa {UK} Limited},
}

@Article{HatemiJ-2004,
  author = {Abdulnasser Hatemi-J},
  date = {2004-07},
  journaltitle = {Economic Modelling},
  title = {Multivariate tests for autocorrelation in the stable and unstable {VAR} models},
  doi = {10.1016/j.econmod.2003.09.005},
  number = {4},
  pages = {661--683},
  volume = {21},
  abstract = {This study investigates the size and power properties of three multivariate tests for autocorrelation, namely portmanteau test, Lagrange multiplier (LM) test and Rao F-test, in the stable and unstable vector autoregressive (VAR) models, with and without autoregressive conditional heteroscedasticity (ARCH) using Monte Carlo experiments. Many combinations of parameters are used in the simulations to cover a wide range of situations in order to make the results more representative. The results of conducted simulations show that all three tests perform relatively well in stable VAR models without ARCH. In unstable VAR models the portmanteau test exhibits serious size distortions. LM and Rao tests perform well in unstable VAR models without ARCH. These results are true, irrespective of sample size or order of autocorrelation. Another clear result that the simulations show is that none of the tests have the correct size when ARCH is present irrespective of VAR models being stable or unstable and regardless of the sample size or order of autocorrelation. The portmanteau test appears to have slightly better power properties than the LM test in almost all scenarios.},
  publisher = {Elsevier {BV}},
}

@Article{Hayes-2009,
  author = {Andrew F. Hayes},
  date = {2009-12},
  journaltitle = {Communication Monographs},
  title = {Beyond {Baron} and {Kenny}: Statistical mediation analysis in the new millennium},
  doi = {10.1080/03637750903310360},
  number = {4},
  pages = {408--420},
  volume = {76},
  abstract = {Understanding communication processes is the goal of most communication researchers. Rarely are we satisfied merely ascertaining whether messages have an effect on some outcome of focus in a specific context. Instead, we seek to understand how such effects come to be. What kinds of causal sequences does exposure to a message initiate? What are the causal pathways through which a message exerts its effect? And what role does communication play in the transmission of the effects of other variables over time and space? Numerous communication models attempt to describe the mechanism through which messages or other communication-related variables transmit their effects or intervene between two other variables in a causal model. The communication literature is replete with tests of such models.
  Over the years, methods used to test such process models have grown in sophistication. An example includes the rise of structural equation modeling (SEM), which allows investigators to examine how well a process model that links some focal variable X to some outcome Y through one or more intervening pathways fits the observed data. Yet frequently, the analytical choices communication researchers make when testing intervening variables models are out of step with advances made in the statistical methods literature. My goal here is to update the field on some of these new advances. While at it, I challenge some conventional wisdom and nudge the field toward a more modern way of thinking about the analysis of intervening variable effects.},
  publisher = {Informa {UK} Limited},
  annotation = {mediation, mediation-bootstrap},
}

@Article{Hayes-Cai-2007,
  author = {Andrew F. Hayes and Li Cai},
  date = {2007-11},
  journaltitle = {Behavior Research Methods},
  title = {Using heteroskedasticity-consistent standard error estimators in {OLS} regression: An introduction and software implementation},
  doi = {10.3758/bf03192961},
  number = {4},
  pages = {709--722},
  volume = {39},
  publisher = {Springer Science and Business Media {LLC}},
  annotation = {regression, regression-hc},
}

@Article{Hedges-Pigott-2004,
  author = {Larry V. Hedges and Therese D. Pigott},
  date = {2004},
  journaltitle = {Psychological Methods},
  title = {The power of statistical tests for moderators in meta-analysis},
  doi = {10.1037/1082-989x.9.4.426},
  issn = {1082-989X},
  number = {4},
  pages = {426--445},
  volume = {9},
  abstract = {Calculation of the statistical power of statistical tests is important in planning and interpreting the results of research studies, including meta-analyses. It is particularly important in moderator analyses in meta-analysis, which are often used as sensitivity analyses to rule out moderator effects but also may have low statistical power. This article describes how to compute statistical power of both fixed- and mixed-effects moderator tests in meta-analysis that are analogous to the analysis of variance and multiple regression analysis for effect sizes. It also shows how to compute power of tests for goodness of fit associated with these models. Examples from a published meta-analysis demonstrate that power of moderator tests and goodness-of-fit tests is not always high.},
  publisher = {American Psychological Association (APA)},
}

@Article{Higgins-Thompson-2002,
  author = {Julian P. T. Higgins and Simon G. Thompson},
  date = {2002-05},
  journaltitle = {Statistics in Medicine},
  title = {Quantifying heterogeneity in a meta-analysis},
  doi = {10.1002/sim.1186},
  issn = {1097-0258},
  number = {11},
  pages = {1539--1558},
  volume = {21},
  abstract = {The extent of heterogeneity in a meta-analysis partly determines the difficulty in drawing overall conclusions. This extent may be measured by estimating a between-study variance, but interpretation is then specific to a particular treatment effect metric. A test for the existence of heterogeneity exists, but depends on the number of studies in the meta-analysis. We develop measures of the impact of heterogeneity on a meta-analysis, from mathematical criteria, that are independent of the number of studies and the treatment effect metric. We derive and propose three suitable statistics: H is the square root of the $\chi^{2}$ heterogeneity statistic divided by its degrees of freedom; $R$ is the ratio of the standard error of the underlying mean from a random effects meta-analysis to the standard error of a fixed effect meta-analytic estimate, and $I^2$ is a transformation of $H$ that describes the proportion of total variation in study estimates that is due to heterogeneity. We discuss interpretation, interval estimates and other properties of these measures and examine them in five example data sets showing different amounts of heterogeneity. We conclude that $H$ and $I^2$, which can usually be calculated for published meta-analyses, are particularly useful summaries of the impact of heterogeneity. One or both should be presented in published meta-analyses in preference to the test for heterogeneity.},
  publisher = {Wiley},
}

@Article{Kauermann-Carroll-2001,
  author = {G{\"o}ran Kauermann and Raymond J. Carroll},
  date = {2001-12},
  journaltitle = {Journal of the American Statistical Association},
  title = {A note on the efficiency of sandwich covariance matrix estimation},
  doi = {10.1198/016214501753382309},
  number = {456},
  pages = {1387--1396},
  volume = {96},
  abstract = {The sandwich estimator, also known as robust covariance matrix estimator, heteroscedasticity-consistent covariance matrix estimate, or empirical covariance matrix estimator, has achieved increasing use in the econometric literature as well as with the growing popularity of generalized estimating equations. Its virtue is that it provides consistent estimates of the covariance matrix for parameter estimates even when the fitted parametric model fails to hold or is not even specified. Surprisingly though, there has been little discussion of properties of the sandwich method other than consistency. We investigate the sandwich estimator in quasi-likelihood models asymptotically, and in the linear case analytically. We show that under certain circumstances when the quasi-likelihood model is correct, the sandwich estimate is often far more variable than the usual parametric variance estimate. The increased variance is a fixed feature of the method and the price that one pays to obtain consistency even when the parametric model fails or when there is heteroscedasticity. We show that the additional variability directly affects the coverage probability of confidence intervals constructed from sandwich variance estimates. In fact, the use of sandwich variance estimates combined with $t$-distribution quantiles gives confidence intervals with coverage probability falling below the nominal value. We propose an adjustment to compensate for this fact.},
  publisher = {Informa {UK} Limited},
  annotation = {regression, regression-hc},
}

@Article{Kenny-Korchmaros-Bolger-2003,
  author = {David A. Kenny and Josephine D. Korchmaros and Niall Bolger},
  date = {2003},
  journaltitle = {Psychological Methods},
  title = {Lower level mediation in multilevel models},
  doi = {10.1037/1082-989x.8.2.115},
  issn = {1082-989X},
  number = {2},
  pages = {115--128},
  volume = {8},
  abstract = {Multilevel models are increasingly used to estimate models for hierarchical and repeated measures data. The authors discuss a model in which there is mediation at the lower level and the mediational links vary randomly across upper level units. One repeated measures example is a case in which a person's daily stressors affect his or her coping efforts, which affect his or her mood, and both links vary randomly across persons. Where there is mediation at the lower level and the mediational links vary randomly across upper level units, the formulas for the indirect effect and its standard error must be modified to include the covariance between the random effects. Because no standard method can estimate such a model, the authors developed an ad hoc method that is illustrated with real and simulated data. Limitations of this method and characteristics of an ideal method are discussed. },
  publisher = {American Psychological Association (APA)},
}

@Article{Kessler-Berglund-Dewit-etal-2002,
  author = {Ronald C. Kessler and Patricia A. Berglund and David J. Dewit and T. Bedirhan {\"U}st{\"u}n and Philip S. Wang and Hans?Ulrich W{\"\i}ttchen},
  date = {2002-08},
  journaltitle = {International Journal of Methods in Psychiatric Research},
  title = {Distinguishing generalized anxiety disorder from major depression: Prevalence and impairment from current pure and comorbid disorders in the {US} and {Ontario}},
  doi = {10.1002/mpr.128},
  issn = {1557-0657},
  number = {3},
  pages = {99--111},
  volume = {11},
  abstract = {Estimation of comparative disease burden in epidemiological surveys is complicated by the fact that high comorbidities exist among many chronic conditions. The easiest way to take comorbidity into consideration is to distinguish between pure and comorbid conditions and to evaluate the incremental effects of comorbid conditions in prediction equations. This approach is illustrated here in an analysis of the effects of pure and comorbid major depression (MD) and generalized anxiety disorder (GAD) on a number of different measures of role impairment in the US National Comorbidity Survey (NCS) and the Mental Health Supplement to the Ontario (Canada) Health Survey (the Supplement). Pure MD and pure GAD were found to have roughly equal independent associations with role impairments. The incremental effects of having comorbid MD and GAD were found to vary depending on the outcome under investigation. The paper closes with a discussion of the methodological complexities associated with generalizing to comorbidities that involve rare conditions or more than two disorders.},
  publisher = {Wiley},
}

@Article{Kessler-Gruber-Hettema-etal-2008,
  author = {Ronald C. Kessler and M. Gruber and J. M. Hettema and I. Hwang and N. Sampson and K. A. Yonkers},
  date = {2008},
  journaltitle = {Psychological Medicine},
  title = {Co-morbid major depression and generalized anxiety disorders in the {National Comorbidity Survey} follow-up},
  doi = {10.1017/s0033291707002012},
  issn = {1469-8978},
  number = {3},
  pages = {365--374},
  volume = {38},
  abstract = {Background: Although generalized anxiety disorder (GAD) and major depressive episode (MDE) are known to be highly co-morbid, little prospective research has examined whether these two disorders predict the subsequent first onset or persistence of the other or the extent to which other predictors explain the time-lagged associations between GAD and MDE. Method: Data were analyzed from the nationally representative two-wave panel sample of 5001 respondents who participated in the 1990-1992 National Comorbidity Survey (NCS) and the 2001-2003 NCS follow-up survey. Both surveys assessed GAD and MDE. The baseline NCS also assessed three sets of risk factors that are considered here: childhood adversities, parental history of mental-substance disorders, and respondent personality. Results: Baseline MDE significantly predicted subsequent GAD onset but not persistence. Baseline GAD significantly predicted subsequent MDE onset and persistence. The associations of each disorder with the subsequent onset of the other attenuated with time since onset of the temporally primary disorder, but remained significant for over a decade after this onset. The risk factors predicted onset more than persistence. Meaningful variation was found in the strength and consistency of associations between risk factors and the two disorders. Controls for risk factors did not substantially reduce the net cross-lagged associations of the disorders with each other. Conclusions: The existence of differences in risk factors for GAD and MDE argues against the view that the two disorders are merely different manifestations of a single underlying internalizing syndrome or that GAD is merely a prodrome, residual, or severity marker of MDE.},
  publisher = {Cambridge University Press (CUP)},
}

@Article{Koob-LeMoal-2008,
  author = {George F. Koob and Michel {Le Moal}},
  date = {2008-01},
  journaltitle = {Annual Review of Psychology},
  title = {Addiction and the brain antireward system},
  doi = {10.1146/annurev.psych.59.103006.093548},
  issn = {1545-2085},
  number = {1},
  pages = {29--53},
  volume = {59},
  abstract = {A neurobiological model of the brain emotional systems has been proposed to explain the persistent changes in motivation that are associated with vulnerability to relapse in addiction, and this model may generalize to other psychopathology associated with dysregulated motivational systems. In this framework, addiction is conceptualized as a cycle of decreased function of brain reward systems and recruitment of antireward systems that progressively worsen, resulting in the compulsive use of drugs. Counteradaptive processes, such as opponent process, that are part of the normal homeostatic limitation of reward function fail to return within the normal homeostatic range and are hypothesized to repeatedly drive the allostatic state. Excessive drug taking thus results in not only the short-term amelioration of the reward deficit but also suppression of the antireward system. However, in the long term, there is worsening of the underlying neurochemical dysregulations that ultimately form an allostatic state (decreased dopamine and opioid peptide function, increased corticotropin-releasing factor activity). This allostatic state is hypothesized to be reflected in a chronic deviation of reward set point that is fueled not only by dysregulation of reward circuits per se but also by recruitment of brain and hormonal stress responses. Vulnerability to addiction may involve genetic comorbidity and developmental factors at the molecular, cellular, or neurocircuitry levels that sensitize the brain antireward systems.},
  publisher = {Annual Reviews},
}

@Article{Krull-MacKinnon-2001,
  author = {Jennifer L. Krull and David P. MacKinnon},
  date = {2001-04},
  journaltitle = {Multivariate Behavioral Research},
  title = {Multilevel modeling of individual and group level mediated effects},
  doi = {10.1207/s15327906mbr3602_06},
  issn = {1532-7906},
  number = {2},
  pages = {249--277},
  volume = {36},
  abstract = {This article combines procedures for single-level mediational analysis with multilevel modeling techniques in order to appropriately test mediational effects in clustered data. A simulation study compared the performance of these multilevel mediational models with that of single-level mediational models in clustered data with individual- or group-level initial independent variables, individual- or group-level mediators, and individual level outcomes. The standard errors of mediated effects from the multilevel solution were generally accurate, while those from the single-level procedure were downwardly biased, often by 20\% or more. The multilevel advantage was greatest in those situations involving group-level variables, larger group sizes, and higher intraclass correlations in mediator and outcome variables. Multilevel mediational modeling methods were also applied to data from a preventive intervention designed to reduce intentions to use steroids among players on high school football teams. This example illustrates differences between single-level and multilevel mediational modeling in real-world clustered data and shows how the multilevel technique may lead to more accurate results.},
  publisher = {Informa UK Limited},
}

@Article{Leventhal-BrooksGunn-2000,
  author = {Tama Leventhal and Jeanne Brooks-Gunn},
  date = {2000},
  journaltitle = {Psychological Bulletin},
  title = {The neighborhoods they live in: The effects of neighborhood residence on child and adolescent outcomes.},
  doi = {10.1037/0033-2909.126.2.309},
  issn = {0033-2909},
  number = {2},
  pages = {309--337},
  volume = {126},
  abstract = {Provides a comprehensive review of research on the effects of neighborhood residence on child and adolescent well-being. The 1st section reviews key methodological issues. The following section considers links between neighborhood characteristics and child outcomes and suggests the importance of high SES for achievement and low SES and residential instability for behavioral/emotional outcomes. The third section identifies 3 pathways (institutional resources, relationships, and norms/collective efficacy) through which neighborhoods might influence development, and which represent an extension of models identified by C. Jencks and S. Mayer (1990) and R. J. Sampson (1992). The models provide a theoretical base for studying neighborhood mechanisms and specify different levels (individual, family, school, peer, community) at which processes may operate. Implications for an emerging developmental framework for research on neighborhoods are discussed.},
  publisher = {American Psychological Association (APA)},
}

@Article{Long-Ervin-2000,
  author = {J. Scott Long and Laurie H. Ervin},
  date = {2000-08},
  journaltitle = {The American Statistician},
  title = {Using heteroscedasticity consistent standard errors in the linear regression model},
  doi = {10.1080/00031305.2000.10474549},
  number = {3},
  pages = {217--224},
  volume = {54},
  publisher = {Informa {UK} Limited},
  annotation = {regression, regression-hc},
}

@Article{Lucas-Fujita-2000,
  author = {Richard E. Lucas and Frank Fujita},
  date = {2000-12},
  journaltitle = {Journal of Personality and Social Psychology},
  title = {Factors influencing the relation between extraversion and pleasant affect},
  doi = {10.1037/0022-3514.79.6.1039},
  issn = {0022-3514},
  number = {6},
  pages = {1039--1056},
  volume = {79},
  abstract = {Factors that influence the relation between extraversion and pleasant affect were examined in 6 studies. In Studies 1 through 5, the authors used structural equation modeling techniques to test whether different extraversion and pleasant affect scales and the use of multiple methods of assessment influenced the strength of the relation. In Study 6, the authors conducted a meta-analysis of previous literature to calculate an average effect size and to assess the influence of moderator variables. Results from both the structural equation models and the meta-analysis showed that with only a few exceptions, the use of different extraversion and affect scales resulted in moderate to strong correlations. The use of ``on-line'' methods of mood assessment (moment reports or daily-diary reports) resulted in lower and more homogeneous correlations than did the use of global, retrospective measures of mood.},
  publisher = {American Psychological Association (APA)},
}

@Article{Ludtke-Marsh-Robitzsch-etal-2008,
  author = {Oliver L{\"u}dtke and Herbert W. Marsh and Alexander Robitzsch and Ulrich Trautwein and Tihomir Asparouhov and Bengt Muth{\a'e}n},
  date = {2008-09},
  journaltitle = {Psychological Methods},
  title = {The multilevel latent covariate model: A new, more reliable approach to group-level effects in contextual studies},
  doi = {10.1037/a0012869},
  issn = {1082-989X},
  number = {3},
  pages = {203--229},
  volume = {13},
  abstract = {In multilevel modeling (MLM), group-level (L2) characteristics are often measured by aggregating individual-level (L1) characteristics within each group so as to assess contextual effects (e.g., group-average effects of socioeconomic status, achievement, climate). Most previous applications have used a multilevel manifest covariate (MMC) approach, in which the observed (manifest) group mean is assumed to be perfectly reliable. This article demonstrates mathematically and with simulation results that this MMC approach can result in substantially biased estimates of contextual effects and can substantially underestimate the associated standard errors, depending on the number of L1 individuals per group, the number of groups, the intraclass correlation, the sampling ratio (the percentage of cases within each group sampled), and the nature of the data. To address this pervasive problem, the authors introduce a new multilevel latent covariate (MLC) approach that corrects for unreliability at L2 and results in unbiased estimates of L2 constructs under appropriate conditions. However, under some circumstances when the sampling ratio approaches 100\%, the MMC approach provides more accurate estimates. Based on 3 simulations and 2 real-data applications, the authors evaluate the MMC and MLC approaches and suggest when researchers should most appropriately use one, the other, or a combination of both approaches.},
  publisher = {American Psychological Association (APA)},
}

@Article{MacKinnon-Fritz-Williams-etal-2007,
  author = {David P. MacKinnon and Matthew S. Fritz and Jason Williams and Chondra M. Lockwood},
  date = {2007-08},
  journaltitle = {Behavior Research Methods},
  title = {Distribution of the product confidence limits for the indirect effect: Program {PRODCLIN}},
  doi = {10.3758/bf03193007},
  number = {3},
  pages = {384--389},
  volume = {39},
  abstract = {This article describes a program, PRODCLIN (distribution of the PRODuct Confidence Limits for INdirect effects), written for SAS, SPSS, and R, that computes confidence limits for the product of two normal random variables. The program is important because it can be used to obtain more accurate confidence limits for the indirect effect, as demonstrated in several recent articles (MacKinnon, Lockwood, \& Williams, 2004; Pituch, Whittaker, \& Stapleton, 2005). Tests of the significance of and confidence limits for indirect effects based on the distribution of the product method have more accurate Type I error rates and more power than other, more commonly used tests. Values for the two paths involved in the indirect effect and their standard errors are entered in the PRODCLIN program, and distribution of the product confidence limits are computed. Several examples are used to illustrate the PRODCLIN program. The PRODCLIN programs in rich text format may be downloaded from www.psychonomic.org/archive.},
  publisher = {Springer Science and Business Media {LLC}},
  annotation = {mediation, mediation-prodclin},
}

@Article{MacKinnon-Krull-Lockwood-2000,
  author = {David P. MacKinnon and Jennifer L. Krull and Chondra M. Lockwood},
  date = {2000},
  journaltitle = {Prevention Science},
  title = {Equivalence of the mediation, confounding and suppression effect},
  doi = {10.1023/a:1026595011371},
  issn = {1389-4986},
  number = {4},
  pages = {173--181},
  volume = {1},
  abstract = {This paper describes the statistical similarities among mediation, confounding, and suppression. Each is quantified by measuring the change in the relationship between an independent and a dependent variable after adding a third variable to the analysis. Mediation and confounding are identical statistically and can be distinguished only on conceptual grounds. Methods to determine the confidence intervals for confounding and suppression effects are proposed based on methods developed for mediated effects. Although the statistical estimation of effects and standard errors is the same, there are important conceptual differences among the three types of effects.},
  publisher = {Springer Science and Business Media LLC},
}

@Article{MacKinnon-Lockwood-Hoffman-etal-2002,
  author = {David P. MacKinnon and Chondra M. Lockwood and Jeanne M. Hoffman and Stephen G. West and Virgil Sheets},
  date = {2002},
  journaltitle = {Psychological Methods},
  title = {A comparison of methods to test mediation and other intervening variable effects},
  doi = {10.1037/1082-989x.7.1.83},
  number = {1},
  pages = {83--104},
  volume = {7},
  abstract = {A Monte Carlo study compared 14 methods to test the statistical significance of the intervening variable effect. An intervening variable (mediator) transmits the effect of an independent variable to a dependent variable. The commonly used R. M. Baron and D. A. Kenny (1986) approach has low statistical power. Two methods based on the distribution of the product and 2 difference-in-coefficients methods have the most accurate Type I error rates and greatest statistical power except in 1 important case in which Type I error rates are too high. The best balance of Type I error and statistical power across all cases is the test of the joint significance of the two effects comprising the intervening variable effect.},
  publisher = {American Psychological Association ({APA})},
  annotation = {mediation, mediation-causalsteps, mediation-jointtest, mediation-prodclin},
}

@Article{MacKinnon-Lockwood-Williams-2004,
  author = {David P. MacKinnon and Chondra M. Lockwood and Jason Williams},
  date = {2004-01},
  journaltitle = {Multivariate Behavioral Research},
  title = {Confidence limits for the indirect effect: Distribution of the product and resampling methods},
  doi = {10.1207/s15327906mbr3901_4},
  number = {1},
  pages = {99--128},
  volume = {39},
  abstract = {The most commonly used method to test an indirect effect is to divide the estimate of the indirect effect by its standard error and compare the resulting z statistic with a critical value from the standard normal distribution. Confidence limits for the indirect effect are also typically based on critical values from the standard normal distribution. This article uses a simulation study to demonstrate that confidence limits are imbalanced because the distribution of the indirect effect is normal only in special cases. Two alternatives for improving the performance of confidence limits for the indirect effect are evaluated: (a) a method based on the distribution of the product of two normal random variables, and (b) resampling methods. In Study 1, confidence limits based on the distribution of the product are more accurate than methods based on an assumed normal distribution but confidence limits are still imbalanced. Study 2 demonstrates that more accurate confidence limits are obtained using resampling methods, with the bias-corrected bootstrap the best method overall.},
  publisher = {Informa {UK} Limited},
  annotation = {mediation, mediation-bootstrap, mediation-montecarlo, mediation-prodclin},
}

@Article{Maxwell-Cole-2007,
  author = {Scott E. Maxwell and David A. Cole},
  date = {2007},
  journaltitle = {Psychological Methods},
  title = {Bias in cross-sectional analyses of longitudinal mediation},
  doi = {10.1037/1082-989x.12.1.23},
  number = {1},
  pages = {23--44},
  volume = {12},
  abstract = {Most empirical tests of mediation utilize cross-sectional data despite the fact that mediation consists of causal processes that unfold over time. The authors considered the possibility that longitudinal mediation might occur under either of two different models of change: (a) an autoregressive model or (b) a random effects model. For both models, the authors demonstrated that cross-sectional approaches to mediation typically generate substantially biased estimates of longitudinal parameters even under the ideal conditions when mediation is complete. In longitudinal models where variable M completely mediates the effect of X on Y, cross-sectional estimates of the direct effect of X on Y, the indirect effect of X on Y through M, and the proportion of the total effect mediated by M are often highly misleading.},
  publisher = {American Psychological Association ({APA})},
  keywords = {mediation, direct effect, indirect effect, cross-sectional designs, longitudinal designs},
}

@Article{McArdle-2009,
  author = {John J. McArdle},
  date = {2009-01},
  journaltitle = {Annual Review of Psychology},
  title = {Latent variable modeling of differences and changes with longitudinal data},
  doi = {10.1146/annurev.psych.60.110707.163612},
  number = {1},
  pages = {577--605},
  volume = {60},
  abstract = {This review considers a common question in data analysis: What is the most useful way to analyze longitudinal repeated measures data? We discuss some contemporary forms of structural equation models (SEMs) based on the inclusion of latent variables. The specific goals of this review are to clarify basic SEM definitions, consider relations to classical models, focus on testable features of the new models, and provide recent references to more complete presentations. A broader goal is to illustrate why so many researchers are enthusiastic about the SEM approach to data analysis. We first outline some classic problems in longitudinal data analysis, consider definitions of differences and changes, and raise issues about measurement errors. We then present several classic SEMs based on the inclusion of invariant common factors and explain why these are so important. This leads to newer SEMs based on latent change scores, and we explain why these are useful.},
  publisher = {Annual Reviews},
  keywords = {linear structural equations, repeated measures},
}

@Article{Mehta-Neale-2005,
  author = {Paras D. Mehta and Michael C. Neale},
  date = {2005-09},
  journaltitle = {Psychological Methods},
  title = {People are variables too: Multilevel structural equations modeling.},
  doi = {10.1037/1082-989x.10.3.259},
  issn = {1082-989X},
  number = {3},
  pages = {259--284},
  volume = {10},
  abstract = {The article uses confirmatory factor analysis (CFA) as a template to explain didactically multilevel structural equation models (ML-SEM) and to demonstrate the equivalence of general mixed-effects models and ML-SEM. An intuitively appealing graphical representation of complex ML-SEMs is introduced that succinctly describes the underlying model and its assumptions. The use of definition variables (i.e., observed variables used to fix model parameters to individual specific data values) is extended to the case of ML-SEMs for clustered data with random slopes. Empirical examples of multilevel CFA and ML-SEM with random slopes are provided along with scripts for fitting such models in SAS Proc Mixed, Mplus, and Mx. Methodological issues regarding estimation of complex ML-SEMs and the evaluation of model fit are discussed. Further potential applications of ML-SEMs are explored.},
  publisher = {American Psychological Association (APA)},
}

@Article{Molenaar-2004,
  author = {Peter C. M. Molenaar},
  date = {2004-10},
  journaltitle = {Measurement: Interdisciplinary Research \& Perspective},
  title = {A manifesto on psychology as idiographic science: Bringing the person back into scientific psychology, this time forever},
  doi = {10.1207/s15366359mea0204_1},
  issn = {1536-6359},
  number = {4},
  pages = {201--218},
  volume = {2},
  abstract = {Psychology is focused on variation between cases (interindividual variation). Results thus obtained are considered to be generalizable to the understanding and explanation of variation within single cases (intraindividual variation). It is indicated, however, that the direct consequences of the classical ergodic theorems for psychology and psychometrics invalidate this conjectured generalizability: only under very strict conditions-which are hardly obtained in real psychological processes-can a generalization be made from a structure of interindividual variation to the analogous structure of intraindividual variation. Illustrations of the lack of this generalizability are given in the contexts of psychometrics, developmental psychology, and personality theory.},
  publisher = {Informa UK Limited},
}

@Article{Molenaar-Campbell-2009,
  author = {Peter C. M. Molenaar and Cynthia G. Campbell},
  date = {2009-04},
  journaltitle = {Current Directions in Psychological Science},
  title = {The new person-specific paradigm in psychology},
  doi = {10.1111/j.1467-8721.2009.01619.x},
  issn = {1467-8721},
  number = {2},
  pages = {112--117},
  volume = {18},
  abstract = {Most research methodology in the behavioral sciences employs interindividual analyses, which provide information about the state of affairs of the population. However, as shown by classical mathematical-statistical theorems (the ergodic theorems), such analyses do not provide information for, and cannot be applied at, the level of the individual, except on rare occasions when the processes of interest meet certain stringent conditions. When psychological processes violate these conditions, the interindividual analyses that are now standardly applied have to be replaced by analysis of intraindividual variation in order to obtain valid results. Two illustrations involving analysis of intraindividual variation of personality and emotional processes are given.},
  publisher = {SAGE Publications},
}

@Article{Morris-Silk-Steinberg-etal-2007,
  author = {Amanda Sheffield Morris and Jennifer S. Silk and Laurence Steinberg and Sonya S. Myers and Lara Rachel Robinson},
  date = {2007-04},
  journaltitle = {Social Development},
  title = {The role of the family context in the development of emotion regulation},
  doi = {10.1111/j.1467-9507.2007.00389.x},
  issn = {1467-9507},
  number = {2},
  pages = {361--388},
  volume = {16},
  abstract = {This article reviews current literature examining associations between components of the family context and children and adolescents' emotion regulation (ER). The review is organized around a tripartite model of familial influence. Firstly, it is posited that children learn about ER through observational learning, modeling and social referencing. Secondly, parenting practices specifically related to emotion and emotion management affect ER. Thirdly, ER is affected by the emotional climate of the family via parenting style, the attachment relationship, family expressiveness and the marital relationship. The review ends with discussions regarding the ways in which child characteristics such as negative emotionality and gender affect ER, how socialization practices change as children develop into adolescents, and how parent characteristics such as mental health affect the socialization of ER.},
  publisher = {Wiley},
}

@Article{Nylund-Asparouhov-Muthen-2007,
  author = {Karen L. Nylund and Tihomir Asparouhov and Bengt O. Muth{\a'e}n},
  date = {2007-10},
  journaltitle = {Structural Equation Modeling: A Multidisciplinary Journal},
  title = {Deciding on the number of classes in latent class analysis and growth mixture modeling: A monte carlo simulation study},
  doi = {10.1080/10705510701575396},
  issn = {1532-8007},
  number = {4},
  pages = {535--569},
  volume = {14},
  abstract = {Mixture modeling is a widely applied data analysis technique used to identify unobserved heterogeneity in a population. Despite mixture models' usefulness in practice, one unresolved issue in the application of mixture models is that there is not one commonly accepted statistical indicator for deciding on the number of classes in a study population. This article presents the results of a simulation study that examines the performance of likelihood-based tests and the traditionally used Information Criterion (ICs) used for determining the number of classes in mixture modeling. We look at the performance of these tests and indexes for 3 types of mixture models: latent class analysis (LCA), a factor mixture model (FMA), and a growth mixture models (GMM). We evaluate the ability of the tests and indexes to correctly identify the number of classes at three different sample sizes ($n = 200, 500, 1,000$). Whereas the Bayesian Information Criterion performed the best of the ICs, the bootstrap likelihood ratio test proved to be a very consistent indicator of classes across all of the models considered.},
  publisher = {Informa UK Limited},
}

@Article{Odgers-Mulvey-Skeem-etal-2009,
  author = {Candice L. Odgers and Edward P. Mulvey and Jennifer L. Skeem and William Gardner and Charles W. Lidz and Carol Schubert},
  date = {2009-05},
  journaltitle = {American Journal of Psychiatry},
  title = {Capturing the ebb and flow of psychiatric symptoms with dynamical systems models},
  doi = {10.1176/appi.ajp.2008.08091398},
  issn = {1535-7228},
  number = {5},
  pages = {575--582},
  volume = {166},
  abstract = {Objective: Psychiatric symptoms play a crucial role in psychology and psychiatry. However, little is known about how dimensions of symptoms---other than symptom level---relate to psychiatric outcomes. Until recently, methods for measuring dynamic aspects of symptoms have not been available to clinicians or researchers. The authors sought to test whether systematic patterns of change in psychiatric symptoms can be recovered across weekly assessments of individuals at high risk for violence. A secondary objective was to explore whether dynamic features of symptoms (specifically, oscillation speed and dysregulation) are concurrently associated with violence, an important indicator of functional impairment for these individuals. Method: Participants ($N = 132$) were drawn from a sample of patients evaluated at the emergency room of an urban psychiatric hospital. Patients actuarially classified as being at high risk for violence were eligible for participation in the study. Participants and collateral informants were interviewed weekly for 26 weeks following an acute psychiatric evaluation. Psychiatric symptoms were assessed using the Brief Symptom Inventory. Measures of symptom fluctuation and regulation were derived using dynamical systems models. Involvement in violence was assessed using self, informant, and official reports. Results: Individuals' symptom dynamics were recovered by a linear oscillator model that described how quickly symptoms oscillated and whether symptoms were amplifying or moving back toward equilibrium across time. Patterns of rapid symptom fluctuation and symptom amplification were concurrently associated with violence. Conclusions: Psychiatric researchers and clinicians have long been interested in adopting more dynamic approaches to understanding symptom change. This study is the first to demonstrate that systematic fluctuations in symptom patterns may be captured by dynamic models. Moreover, the concurrent association between symptom dynamics and violence suggests avenues for future research to test how features of symptom fluctuation could affect behavior.},
  publisher = {American Psychiatric Association Publishing},
}

@Article{Oud-Jansen-2000,
  author = {Johan H. L. Oud and Robert A. R. G. Jansen},
  date = {2000-06},
  journaltitle = {Psychometrika},
  title = {Continuous time state space modeling of panel data by means of {SEM}},
  doi = {10.1007/bf02294374},
  number = {2},
  pages = {199--215},
  volume = {65},
  abstract = {Maximum likelihood parameter estimation of the continuous time linear stochastic state space model is considered on the basis of largeN discrete time data using a structural equation modeling (SEM) program. Random subject effects are allowed to be part of the model. The exact discrete model (EDM) is employed which links the discrete time model parameters to the underlying continuous time model parameters by means of nonlinear restrictions. The EDM is generalized to cover not only time-invariant parameters but also the cases of stepwise time-varying (piecewise time-invariant) parameters and parameters varying continuously over time according to a general polynomial scheme. The identification of the continuous time parameters is discussed and an educational example is presented.},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Patrick-Maggs-2007,
  author = {Megan E. Patrick and Jennifer L. Maggs},
  date = {2007-07},
  journaltitle = {Journal of Adolescence},
  title = {Short-term changes in plans to drink and importance of positive and negative alcohol consequences},
  doi = {10.1016/j.adolescence.2007.06.002},
  issn = {1095-9254},
  number = {3},
  pages = {307--321},
  volume = {31},
  abstract = {Experienced consequences predicted short-term changes in alcohol use plans and perceptions of the importance of alcohol-related consequences. Participants were 176 traditionally aged first-year university students who completed a 10-week telephone diary study (total weeks = 1735). In multi-level models, men and students who experienced more positive and negative consequences on average planned to drink more and rated avoiding negative consequences as less important. Students who experienced more positive consequences rated them as more important (between-person analyses). Following weeks of experiencing relatively more positive drinking consequences, students planned to drink more and rated experiencing positive consequences as more important for the subsequent week (within-person analyses). Challenges for intervening in the ongoing formation of anticipatory cognitions regarding alcohol use are discussed.},
  publisher = {Wiley},
}

@Article{Patrick-Maggs-2009,
  author = {Megan E. Patrick and Jennifer L. Maggs},
  date = {2009-11},
  journaltitle = {Journal of Adolescence},
  title = {Profiles of motivations for alcohol use and sexual behavior among first-year university students},
  doi = {10.1016/j.adolescence.2009.10.003},
  issn = {1095-9254},
  number = {5},
  pages = {755--765},
  volume = {33},
  abstract = {The links between motivations for alcohol use and for sexual behaviors are not well understood. Latent profile analysis was used to identify drinking motivational profiles (based on motivations for: fun/social, relaxation/coping, image, sex; motivations against: physical, behavioral) and sex motivational profiles (motivations for: enhancement, intimacy, coping; motivations against: not ready, health, values) among college students (N = 227, 51\% male). Latent profiles for drinking were: low for/high against drinking (34\%), average drinking motives (53\%), and high for/low against drinking (13\%). Profiles for sex were: low for/high against sex (35\%), high for/low against sex (42\%), and high for with coping/moderate against sex (23\%). Motivational profiles were related across behaviors. Drinking motivational profiles were associated with alcohol use and psychosocial adjustment; sex motivational profiles were associated with sexual experiences. Distinct profiles of motivations support the need for differentiated intervention programs targeting individuals with different patterns of reasons for engaging in risk behaviors during late adolescence.},
  publisher = {Wiley},
}

@Article{Peugh-Enders-2004,
  author = {James L. Peugh and Craig K. Enders},
  date = {2004-12},
  journaltitle = {Review of Educational Research},
  title = {Missing data in educational research: A review of reporting practices and suggestions for improvement},
  doi = {10.3102/00346543074004525},
  number = {4},
  pages = {525--556},
  volume = {74},
  publisher = {American Educational Research Association ({AERA})},
  abstract = {Missing data analyses have received considerable recent attention in the methodological literature, and two ``modern'' methods, multiple imputation and maximum likelihood estimation, are recommended. The goals of this article are to (a) provide an overview of missing-data theory, maximum likelihood estimation, and multiple imputation; (b) conduct a methodological review of missing-data reporting practices in 23 applied research journals; and (c) provide a demonstration of multiple imputation and maximum likelihood estimation using the Longitudinal Study of American Youth data. The results indicated that explicit discussions of missing data increased substantially between 1999 and 2003, but the use of maximum likelihood estimation or multiple imputation was rare; the studies relied almost exclusively on listwise and pairwise deletion.},
  keywords = {EM algorithm, maximum likelihood estimation, missing data, multiple imputation, NORM},
}

@Article{Preacher-Curran-Bauer-2006,
  author = {Kristopher J. Preacher and Patrick J. Curran and Daniel J. Bauer},
  date = {2006-12},
  journaltitle = {Journal of Educational and Behavioral Statistics},
  title = {Computational tools for probing interactions in multiple linear regression, multilevel modeling, and latent curve analysis},
  doi = {10.3102/10769986031004437},
  issn = {1935-1054},
  number = {4},
  pages = {437--448},
  volume = {31},
  abstract = {Simple slopes, regions of significance, and confidence bands are commonly used to evaluate interactions in multiple linear regression (MLR) models, and the use of these techniques has recently been extended to multilevel or hierarchical linear modeling (HLM) and latent curve analysis (LCA). However, conducting these tests and plotting the conditional relations is often a tedious and error-prone task. This article provides an overview of methods used to probe interaction effects and describes a unified collection of freely available online resources that researchers can use to obtain significance tests for simple slopes, compute regions of significance, and obtain confidence bands for simple slopes across the range of the moderator in the MLR, HLM, and LCA contexts. Plotting capabilities are also provided.},
  publisher = {American Educational Research Association (AERA)},
}

@Article{Preacher-Hayes-2004,
  author = {Kristopher J. Preacher and Andrew F. Hayes},
  date = {2004-11},
  journaltitle = {Behavior Research Methods, Instruments, \& Computers},
  title = {{SPSS} and {SAS} procedures for estimating indirect effects in simple mediation models},
  doi = {10.3758/bf03206553},
  number = {4},
  pages = {717--731},
  volume = {36},
  abstract = {Researchers often conduct mediation analysis in order to indirectly assess the effect of a proposed cause on some outcome through a proposed mediator. The utility of mediation analysis stems from its ability to go beyond the merely descriptive to a more functional understanding of the relationships among variables. A necessary component of mediation is a statistically and practically significant indirect effect. Although mediation hypotheses are frequently explored in psychological research, formal significance tests of indirect effects are rarely conducted. After a brief overview of mediation, we argue the importance of directly testing the significance of indirect effects and provide SPSS and SAS macros that facilitate estimation of the indirect effect with a normal theory approach and a bootstrap approach to obtaining confidence intervals, as well as the traditional approach advocated by Baron and Kenny (1986). We hope that this discussion and the macros will enhance the frequency of formal mediation tests in the psychology literature. Electronic copies of these macros may be downloaded from the Psychonomic Society's Web archive at www.psychonomic.org/archive/.},
  publisher = {Springer Science and Business Media {LLC}},
  keywords = {life satisfaction, indirect effect, mediation analysis, cognitive therapy, Sobel test},
  annotation = {mediation, mediation-delta, mediation-bootstrap},
}

@Article{Preacher-Hayes-2008,
  author = {Kristopher J. Preacher and Andrew F. Hayes},
  date = {2008-08},
  journaltitle = {Behavior Research Methods},
  title = {Asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models},
  doi = {10.3758/brm.40.3.879},
  number = {3},
  pages = {879--891},
  volume = {40},
  abstract = {Hypotheses involving mediation are common in the behavioral sciences. Mediation exists when a predictor affects a dependent variable indirectly through at least one intervening variable, or mediator. Methods to assess mediation involving multiple simultaneous mediators have received little attention in the methodological literature despite a clear need. We provide an overview of simple and multiple mediation and explore three approaches that can be used to investigate indirect processes, as well as methods for contrasting two or more mediators within a single model. We present an illustrative example, assessing and contrasting potential mediators of the relationship between the helpfulness of socialization agents and job satisfaction. We also provide SAS and SPSS macros, as well as Mplus and LISREL syntax, to facilitate the use of these methods in applications.},
  publisher = {Springer Science and Business Media {LLC}},
  keywords = {indirect effect, structural equation modeling, residual covariance, total indirect effect, multiple mediator model},
  annotation = {mediation, mediation-bootstrap},
}

@Article{Raghunathan-Lepkowski-vanHoewyk-etal-2001,
  author = {Trivellore E. Raghunathan and James M. Lepkowski and John {van Hoewyk} and Peter Solenberger},
  date = {2001},
  journaltitle = {Survey Methodology},
  title = {A multivariate technique for multiply imputing missing values using a sequence of regression models},
  number = {1},
  pages = {85--95},
  volume = {27},
  abstract = {This article describes and evaluates a procedure for imputing missing values for a relatively complex data structure when the data are missing at random. The imputations are obtained by fitting a sequence of regression models and drawing values from the corresponding predictive distributions. The types of regression models used are linear, logistic, Poisson, generalized logit or a mixture of these depending on the type of variable being imputed. Two additional common features in the imputation process are incorporated: restriction to a relevant subpopulation for some variables and logical bounds or constraints for the imputed values. The restrictions involve subsetting the sample individuals that satisfy certain criteria while fitting the regression models. The bounds involve drawing values from a truncated predictive distribution. The development of this method was partly motivated by the analysis of two data sets which are used as illustrations. The sequential regression procedure is applied to perform multiple imputation analysis for the two applied problems. The sampling properties of inferences from multiply imputed data sets created using the sequential regression method are evaluated through simulated data sets.},
  keywords = {item nonresponse, missing at random, multiple imputation, nonignorable missing mechanism, regression, sampling properties and simulations},
}

@Article{Raykov-Marcoulides-2004,
  author = {Tenko Raykov and George A. Marcoulides},
  date = {2004-10},
  journaltitle = {Structural Equation Modeling: A Multidisciplinary Journal},
  title = {Using the delta method for approximate interval estimation of parameter functions in {SEM}},
  doi = {10.1207/s15328007sem1104_7},
  issn = {1532-8007},
  number = {4},
  pages = {621--637},
  volume = {11},
  abstract = {In applications of structural equation modeling, it is often desirable to obtain measures of uncertainty for special functions of model parameters. This article provides a didactic discussion of how a method widely used in applied statistics can be employed for approximate standard error and confidence interval evaluation of such functions. The described approach is illustrated with data from a cognitive intervention study, in which it is used to estimate time-invariant reliability in multiwave, multiple indicator models.},
  publisher = {Informa UK Limited},
}

@Article{Reinert-Allen-2007,
  author = {Duane F. Reinert and John P. Allen},
  date = {2007-01},
  journaltitle = {Alcoholism: Clinical and Experimental Research},
  title = {The alcohol use disorders identification test: An update of research findings},
  doi = {10.1111/j.1530-0277.2006.00295.x},
  issn = {1530-0277},
  number = {2},
  pages = {185--199},
  volume = {31},
  abstract = {Background: The Alcohol Use Disorders Identification Test (AUDIT) has been extensively researched to determine its capability to accurately and practically screen for alcohol problems. Methods: During the 5 years since our previous review of the literature, a large number of additional studies have been published on the AUDIT, abbreviated versions of it, its psychometric properties, and the applicability of the AUDIT for a diverse array of populations. The current article summarizes new findings and integrates them with results of previous research. It also suggests some issues that we believe are particularly in need of further study. Results: A growing body of research evidence supports the criterion validity of English version of the AUDIT as a screen for alcohol dependence as well as for less severe alcohol problems. Nevertheless, the cut-points for effective detection of hazardous drinking as well as identification of alcohol dependence or harmful use in women need to be lowered from the originally recommended value of 8 points. The AUDIT-C, the most popular short version of the AUDIT consisting solely of its 3 consumption items, is approximately equal in accuracy to the full AUDIT. Psychometric properties of the AUDIT, such as test-retest reliability and internal consistency, are quite favorable. Continued research is urged to establish the psychometric properties of non-English versions of the AUDIT, use of the AUDIT with adolescents and with older adults, and selective inclusion of alcohol biomarkers with the AUDIT in some instances. Conclusions: Research continues to support use of the AUDIT as a means of screening for the spectrum of alcohol use disorders in various settings and with diverse populations.},
  publisher = {Wiley},
}

@Article{Sakai-MikulichGilbertson-Long-etal-2006,
  author = {Joseph T. Sakai and Susan K. Mikulich-Gilbertson and Robert J. Long and Thomas J. Crowley},
  date = {2006-01},
  journaltitle = {Alcoholism: Clinical and Experimental Research},
  title = {Validity of transdermal alcohol monitoring: Fixed and self-regulated dosing},
  doi = {10.1111/j.1530-0277.2006.00004.x},
  issn = {1530-0277},
  number = {1},
  pages = {26--33},
  volume = {30},
  abstract = {Background: To study the validity of transdermal assessment of alcohol concentration measured by a lightweight, noninvasive device. Methods: Subjects wore a 227-g anklet that sensed transdermal alcohol concentrations (TACs) every 15 to 30 minutes, downloading results to a remote computer each day. Twenty-four subjects entered a laboratory and received a dose of 0, 0.28, or 0.56 g/kg of ethanol. Breath alcohol concentrations (BrAC) and TAC were measured every 15 to 30 minutes Twenty others [10 alcohol dependent (AD) and 10 not (NAD)] in the community who wore the anklet for 8 days kept a drinking log and provided a BrAC sample each day. Results: In the laboratory, no zero-dose subject, and every subject receiving alcohol, had alcohol-positive TACs. The device distinguished low- and high-alcohol-dosing groups using peak ($t_{14} = 3.37$; $ p < 0.01$) and area under the curve ($t_{14} = 3.42$; $p < 0.01$) of TACs. Within dosing groups, average TAC curves were broader (right-shifted) and had lower peaks than average BrAC curves. For community participants, self-reported number of drinks ($t_{18} = -3.77$; $p < 0.01$), area under the TAC curve ($t_{9.5} = -3.56$; $p < 0.01$), and mean TAC ($t_{9.9} = -3.35$; $p < 0.01$) all significantly distinguished the AD and NAD groups. However, individual transdermal readings were not reliably quantitatively equivalent to simultaneously obtained breath results. Conclusions: Within the limits of the laboratory study, the device consistently detected consumption of approximately 2 standard drinks. On average, the device shows discriminative validity as a semiquantitative measure of alcohol consumption but individual readings often are not equivalent to simultaneous BrACs.},
  publisher = {Wiley},
}

@Article{Schafer-Graham-2002,
  author = {Joseph L. Schafer and John W. Graham},
  date = {2002},
  journaltitle = {Psychological Methods},
  title = {Missing data: Our view of the state of the art},
  doi = {10.1037/1082-989x.7.2.147},
  number = {2},
  pages = {147--177},
  volume = {7},
  abstract = {Statistical procedures for missing data have vastly improved, yet misconception and unsound practice still abound. The authors frame the missing-data problem, review methods, offer advice, and raise issues that remain unresolved. They clear up common misunderstandings regarding the missing at random (MAR) concept. They summarize the evidence against older procedures and, with few exceptions, discourage their use. They present, in both technical and practical language, 2 general approaches that come highly recommended: maximum likelihood (ML) and Bayesian multiple imputation (MI). Newer developments are discussed, including some for dealing with missing data that are not MAR. Although not yet in the mainstream, these procedures may eventually extend the ML and MI methods that currently represent the state of the art.},
  publisher = {American Psychological Association ({APA})},
}

@Article{Selig-Preacher-2009,
  author = {James P. Selig and Kristopher J. Preacher},
  date = {2009-06},
  journaltitle = {Research in Human Development},
  title = {Mediation models for longitudinal data in developmental research},
  doi = {10.1080/15427600902911247},
  number = {2-3},
  pages = {144--164},
  volume = {6},
  abstract = {Mediation models are used to describe the mechanism(s) by which one variable influences another. These models can be useful in developmental research to explicate the relationship between variables, developmental processes, or combinations of variables and processes. In this article we describe aspects of mediation effects specific to developmental research. We focus on three central issues in longitudinal mediation models: the theory of change for variables in the model, the role of time in the model, and the types of indirect effects in the model. We use these themes as we describe three different models for examining mediation in longitudinal data.},
  publisher = {Informa {UK} Limited},
}

@Article{Serlin-2000,
  author = {Ronald C. Serlin},
  date = {2000},
  journaltitle = {Psychological Methods},
  title = {Testing for robustness in {Monte Carlo} studies},
  doi = {10.1037/1082-989x.5.2.230},
  number = {2},
  pages = {230--240},
  volume = {5},
  abstract = {Monte Carlo studies provide the information needed to help researchers select appropriate analytical procedures under design conditions in which the underlying assumptions of the procedures are not met. In Monte Carlo studies, the 2 errors that one could commit involve (a) concluding that a statistical procedure is robust when it is not or (b) concluding that it is not robust when it is. In previous attempts to apply standard statistical design principles to Monte Carlo studies, the less severe of these errors has been wrongly designated the Type I error. In this article, a method is presented for controlling the appropriate Type I error rate; the determination of the number of iterations required in a Monte Carlo study to achieve desired power is described; and a confidence interval for a test's true Type I error rate is derived. A robustness criterion is also proposed that is a compromise between W. G. Cochran's (1952) and J. V. Bradley's (1978) criteria.},
  publisher = {American Psychological Association ({APA})},
  annotation = {robustness},
}

@Article{Shadish-Rindskopf-Hedges-2008,
  author = {William R. Shadish and David M. Rindskopf and Larry V. Hedges},
  date = {2008-09},
  journaltitle = {Evidence-Based Communication Assessment and Intervention},
  title = {The state of the science in the meta-analysis of single-case experimental designs},
  doi = {10.1080/17489530802581603},
  issn = {1748-9547},
  number = {3},
  pages = {188--196},
  volume = {2},
  abstract = {The articles in the previous, special issue of Evidence-Based Communication Assessment and Intervention provided an excellent review of the meta-analysis of single-case designs. This article weaves commentary about those articles into a larger narrative about two major lines of attack on this problem: the use of parametric approaches like regression and multilevel modeling, and the development of parametric and nonparametric effect-size estimators. On each of these two topics, we describe an agenda of research topics that need to be addressed; and we also introduce a new effect-size estimator that may prove to be comparable to the usual standardized mean difference statistics (d) widely used in between-groups analysis. The article ends with observations about ways in which developments in the meta-analysis of single-case designs may have far wider implications than previously appreciated.},
  publisher = {Informa UK Limited},
}

@Article{Shiffman-2009,
  author = {Saul Shiffman},
  date = {2009-12},
  journaltitle = {Psychological Assessment},
  title = {Ecological momentary assessment ({EMA}) in studies of substance use},
  doi = {10.1037/a0017074},
  number = {4},
  pages = {486--497},
  volume = {21},
  abstract = {Ecological momentary assessment (EMA) is particularly suitable for studying substance use, because use is episodic and thought to be related to mood and context. This article reviews EMA methods in substance use research, focusing on tobacco and alcohol use and relapse, where EMA has been most applied. Common EMA designs combine event-based reports of substance use with time-based assessments. Approaches to data organization and analysis have been very diverse, particularly regarding their treatment of time. Compliance with signaled assessments is often high. Compliance with recording of substance use appears good but is harder to validate. Treatment applications of EMA are emerging. EMA captures substance use patterns not measured by questionnaires or retrospective data and holds promise for substance use research.},
  publisher = {American Psychological Association ({APA})},
  keywords = {ecological momentary assessment, substance use, drug use, tobacco, alcohol},
}

@Article{Shiffman-Stone-Hufford-2008,
  author = {Saul Shiffman and Arthur A. Stone and Michael R. Hufford},
  date = {2008-04},
  journaltitle = {Annual Review of Clinical Psychology},
  title = {Ecological momentary assessment},
  doi = {10.1146/annurev.clinpsy.3.022806.091415},
  number = {1},
  pages = {1--32},
  volume = {4},
  abstract = {Assessment in clinical psychology typically relies on global retrospective self-reports collected at research or clinic visits, which are limited by recall bias and are not well suited to address how behavior changes over time and across contexts. Ecological momentary assessment (EMA) involves repeated sampling of subjects' current behaviors and experiences in real time, in subjects' natural environments. EMA aims to minimize recall bias, maximize ecological validity, and allow study of microprocesses that influence behavior in real-world contexts. EMA studies assess particular events in subjects' lives or assess subjects at periodic intervals, often by random time sampling, using technologies ranging from written diaries and telephones to electronic diaries and physiological sensors. We discuss the rationale for EMA, EMA designs, methodological and practical issues, and comparisons of EMA and recall data. EMA holds unique promise to advance the science and practice of clinical psychology by shedding light on the dynamics of behavior in real-world settings.},
  publisher = {Annual Reviews},
  keywords = {diary, experience sampling, real-time data capture},
}

@Article{Shrout-Bolger-2002,
  author = {Patrick E. Shrout and Niall Bolger},
  date = {2002},
  journaltitle = {Psychological Methods},
  title = {Mediation in experimental and nonexperimental studies: New procedures and recommendations},
  doi = {10.1037/1082-989x.7.4.422},
  number = {4},
  pages = {422--445},
  volume = {7},
  publisher = {American Psychological Association ({APA})},
  abstract = {Mediation is said to occur when a causal effect of some variable $X$ on an outcome $Y$ is explained by some intervening variable $M$. The authors recommend that with small to moderate samples, bootstrap methods (B. Efron \& R. Tibshirani, 1993) be used to assess mediation. Bootstrap tests are powerful because they detect that the sampling distribution of the mediated effect is skewed away from 0. They argue that R. M. Baron and D. A. Kenny's (1986) recommendation of first testing the $X \to Y$ association for statistical significance should not be a requirement when there is a priori belief that the effect size is small or suppression is a possibility. Empirical examples and computer setups for bootstrap analyses are provided.},
  publisher = {American Psychological Association ({APA})},
  annotation = {mediation, mediation-bootstrap},
}

@Article{Sliwinski-2008,
  author = {Martin J. Sliwinski},
  date = {2008-01},
  journaltitle = {Social and Personality Psychology Compass},
  title = {Measurementburst designs for social health research},
  doi = {10.1111/j.1751-9004.2007.00043.x},
  issn = {1751-9004},
  number = {1},
  pages = {245--261},
  volume = {2},
  abstract = {Questions about variability and change in human behavior lie at the heart of much research in the behavioral sciences. This commentary examines the use of the measurement-burst design (Nesselroade, 1991) as an approach to study within-person processes that transpire over very different temporal intervals. Consisting of repeated bursts of intensive (i.e., daily or momentary) assessments, the burst design can augment the type of information obtained from conventional daily diary and prospective longitudinal designs. We describe how the measurement-burst approach can improve detection of long-term intraindividual change, and how it can be used to study intraindividual variability and change in fine-grained temporal relationships between daily experiences (e.g., hassles) and psychological states (e.g., mood). Some of the difficulties of implementing and analyzing data from measurement-burst designs are discussed.},
  publisher = {Wiley},
}

@Article{Squeglia-Jacobus-Tapert-2009,
  author = {Lindsay M. Squeglia and Joanna Jacobus and Susan F. Tapert},
  date = {2009-01},
  journaltitle = {Clinical EEG and Neuroscience},
  title = {The influence of substance use on adolescent brain development},
  doi = {10.1177/155005940904000110},
  issn = {2169-5202},
  number = {1},
  pages = {31--38},
  volume = {40},
  abstract = {Adolescence is a unique period in neurodevelopment. Alcohol and marijuana use are common. Recent research has indicated that adolescent substance users show abnormalities on measures of brain functioning, which is linked to changes in neurocognition over time. Abnormalities have been seen in brain structure volume, white matter quality, and activation to cognitive tasks, even in youth with as little as 1-2 years of heavy drinking and consumption levels of 20 drinks per month, especially if >4-5 drinks are consumed on a single occasion. Heavy marijuana users show some subtle anomalies too, but generally not the same degree of divergence from demographically similar non-using adolescents. This article reviews the extant literature on neurocognition, brain structure, and brain function in adolescent substance users with an emphasis on the most commonly used substances, and in the context of ongoing neuromaturational processes. Methodological and treatment implications are provided.},
  publisher = {SAGE Publications},
}

@Article{Stattin-Kerr-2000,
  author = {H{\"a}kan Stattin and Margaret Kerr},
  date = {2000-07},
  journaltitle = {Child Development},
  title = {Parental monitoring: A reinterpretation},
  doi = {10.1111/1467-8624.00210},
  issn = {1467-8624},
  number = {4},
  pages = {1072--1085},
  volume = {71},
  abstract = {Monitoring (tracking and surveillance) of children's behavior is considered an essential parenting skill. Numerous studies show that well-monitored youths are less involved in delinquency and other normbreaking behaviors, and scholars conclude that parents should track their children more carefully. This study questions that conclusion. We point out that monitoring measures typically assess parents' knowledge but not its source, and parents could get knowledge from their children's free disclosure of information as well as their own active surveillance efforts. In our study of 703 14-year-olds in central Sweden and their parents, parental knowledge came mainly from child disclosure, and child disclosure was the source of knowledge that was most closely linked to broad and narrow measures of delinquency (normbreaking and police contact). These results held for both children's and parents' reports, for both sexes, and were independent of whether the children were exhibiting problem behavior or not. We conclude that tracking and surveillance is not the best prescription for parental behavior and that a new prescription must rest on an understanding of the factors that determine child disclosure.},
  publisher = {Wiley},
}

@Article{Staudenmayer-Buonaccorsi-2005,
  author = {John Staudenmayer and John P Buonaccorsi},
  date = {2005-09},
  journaltitle = {Journal of the American Statistical Association},
  title = {Measurement error in linear autoregressive models},
  doi = {10.1198/016214504000001871},
  issn = {1537-274X},
  number = {471},
  pages = {841--852},
  volume = {100},
  abstract = {Time series data are often subject to measurement error, usually the result of needing to estimate the variable of interest. Although it is often reasonable to assume that the measurement error is additive (i.e., the estimator is conditionally unbiased for the missing true value), the measurement error variances often vary as a result of changes in the population/process over time and/or changes in sampling effort. In this article we address estimation of the parameters in linear autoregressive models in the presence of additive and uncorrelated measurement errors, allowing heteroscedasticity in the measurement error variances. We establish the asymptotic properties of naive estimators that ignore measurement error and propose an estimator based on correcting the Yule-Walker estimating equations. We also examine a pseudo-likelihood method based on normality assumptions and computed using the Kalman filter. We review other techniques that have been proposed, including two that require no information about the measurement error variances, and compare the various estimators both theoretically and via simulations. The estimator based on corrected estimating equations is easy to obtain and readily accommodates (and is robust to) unequal measurement error variances. Asymptotic calculations and finite-sample simulations show that it is often relatively efficient.},
  publisher = {Informa UK Limited},
}

@Article{Swift-2000,
  author = {Robert Swift},
  date = {2000-04},
  journaltitle = {Alcoholism: Clinical and Experimental Research},
  title = {Transdermal alcohol measurement for estimation of blood alcohol concentration},
  doi = {10.1111/j.1530-0277.2000.tb02006.x},
  issn = {1530-0277},
  number = {4},
  pages = {422--423},
  volume = {24},
  publisher = {Wiley},
}

@Article{Taylor-MacKinnon-Tein-2007,
  author = {Aaron B. Taylor and David P. MacKinnon and Jenn-Yun Tein},
  date = {2007-07},
  journaltitle = {Organizational Research Methods},
  title = {Tests of the three-path mediated effect},
  doi = {10.1177/1094428107300344},
  number = {2},
  pages = {241--269},
  volume = {11},
  abstract = {In a three-path mediational model, two mediators intervene in a series between an independent and a dependent variable. Methods of testing for mediation in such a model are generalized from the more often used single-mediator model. Six such methods are introduced and compared in a Monte Carlo study in terms of their Type I error, power, and coverage. Based on its results, the joint significance test is preferred when only a hypothesis test is of interest. The percentile bootstrap and bias-corrected bootstrap are preferred when a confidence interval on the mediated effect is desired, with the latter having more power but also slightly inflated Type I error in some conditions.},
  publisher = {{SAGE} Publications},
  keywords = {mediation, bootstrapping},
  annotation = {mediation, mediation-bootstrap, mediation-jointtest},
}

@Article{Thompson-Higgins-2002,
  author = {Simon G. Thompson and Julian P. T. Higgins},
  date = {2002-05},
  journaltitle = {Statistics in Medicine},
  title = {How should meta-regression analyses be undertaken and interpreted?},
  doi = {10.1002/sim.1187},
  issn = {1097-0258},
  number = {11},
  pages = {1559--1573},
  volume = {21},
  abstract = {Appropriate methods for meta-regression applied to a set of clinical trials, and the limitations and pitfalls in interpretation, are insufficiently recognized. Here we summarize recent research focusing on these issues, and consider three published examples of meta-regression in the light of this work. One principal methodological issue is that meta-regression should be weighted to take account of both within-trial variances of treatment effects and the residual between-trial heterogeneity (that is, heterogeneity not explained by the covariates in the regression). This corresponds to random effects meta-regression. The associations derived from meta-regressions are observational, and have a weaker interpretation than the causal relationships derived from randomized comparisons. This applies particularly when averages of patient characteristics in each trial are used as covariates in the regression. Data dredging is the main pitfall in reaching reliable conclusions from meta-regression. It can only be avoided by prespecification of covariates that will be investigated as potential sources of heterogeneity. However, in practice this is not always easy to achieve. The examples considered in this paper show the tension between the scientific rationale for using meta-regression and the difficult interpretative problems to which such analyses are prone.},
  publisher = {Wiley},
}

@Article{vanBuuren-Brand-GroothuisOudshoorn-etal-2006,
  author = {Stef {van Buuren} and J. P. L. Brand and C. G. M. Groothuis-Oudshoorn and Donald B. Rubin},
  date = {2006-12},
  journaltitle = {Journal of Statistical Computation and Simulation},
  title = {Fully conditional specification in multivariate imputation},
  doi = {10.1080/10629360600810434},
  number = {12},
  pages = {1049--1064},
  volume = {76},
  abstract = {The use of the Gibbs sampler with fully conditionally specified models, where the distribution of each variable given the other variables is the starting point, has become a popular method to create imputations in incomplete multivariate data. The theoretical weakness of this approach is that the specified conditional densities can be incompatible, and therefore the stationary distribution to which the Gibbs sampler attempts to converge may not exist. This study investigates practical consequences of this problem by means of simulation. Missing data are created under four different missing data mechanisms. Attention is given to the statistical behavior under compatible and incompatible models. The results indicate that multiple imputation produces essentially unbiased estimates with appropriate coverage in the simple cases investigated, even for the incompatible models. Of particular interest is that these results were produced using only five Gibbs iterations starting from a simple draw from observed marginal distributions. It thus appears that, despite the theoretical weaknesses, the actual performance of conditional model specification for multivariate imputation can be quite good, and therefore deserves further study.},
  publisher = {Informa {UK} Limited},
  keywords = {multivariate missing data, multiple imputation, distributional compatibility, Gibbs sampling, simulation, proper imputation},
}

@Article{VandenNoortgate-Onghena-2008,
  author = {Wim {Van den Noortgate} and Patrick Onghena},
  date = {2008-09},
  journaltitle = {Evidence-Based Communication Assessment and Intervention},
  title = {A multilevel meta-analysis of single-subject experimental design studies},
  doi = {10.1080/17489530802505362},
  issn = {1748-9547},
  number = {3},
  pages = {142--151},
  volume = {2},
  abstract = {A multilevel approach is proposed to synthesize the results of single-subject experimental research by combining either raw data or effect-size measures. A multilevel meta-analysis of single-subject data allows us to make inferences not only about the effects for the participants that are investigated, but also about the generality of the effects. It also permits a systematic exploration of the possible moderating effect of study and participant characteristics. We also discuss the possibility of combining single-subject experimental research and group-comparison research in one single meta-analysis, and the difficulties involved, especially in making effect-size measures comparable for both kinds of studies.},
  publisher = {Informa UK Limited},
}

@Article{vanHouwelingen-Arends-Stijnen-2002,
  author = {Hans C. {van Houwelingen} and Lidia R. Arends and Theo Stijnen},
  date = {2002-01},
  journaltitle = {Statistics in Medicine},
  title = {Advanced methods in meta-analysis: multivariate approach and meta-regression},
  doi = {10.1002/sim.1040},
  issn = {1097-0258},
  number = {4},
  pages = {589--624},
  volume = {21},
  abstract = {This tutorial on advanced statistical methods for meta-analysis can be seen as a sequel to the recent Tutorial in Biostatistics on meta-analysis by Normand, which focused on elementary methods. Within the framework of the general linear mixed model using approximate likelihood, we discuss methods to analyse univariate as well as bivariate treatment effects in meta-analyses as well as meta-regression methods. Several extensions of the models are discussed, like exact likelihood, non-normal mixtures and multiple endpoints. We end with a discussion about the use of Bayesian methods in meta-analysis. All methods are illustrated by a meta-analysis concerning the efficacy of BCG vaccine against tuberculosis. All analyses that use approximate likelihood can be carried out by standard software. We demonstrate how the models can be fitted using SAS Proc Mixed.},
  publisher = {Wiley},
}

@Article{Viechtbauer-2007,
  author = {Wolfgang Viechtbauer},
  date = {2007-02},
  journaltitle = {Statistics in Medicine},
  title = {Confidence intervals for the amount of heterogeneity in meta-analysis},
  doi = {10.1002/sim.2514},
  issn = {1097-0258},
  number = {1},
  pages = {37--52},
  volume = {26},
  abstract = {Effect size estimates to be combined in a systematic review are often found to be more variable than one would expect based on sampling differences alone. This is usually interpreted as evidence that the effect sizes are heterogeneous. A random-effects model is then often used to account for the heterogeneity in the effect sizes. A novel method for constructing confidence intervals for the amount of heterogeneity in the effect sizes is proposed that guarantees nominal coverage probabilities even in small samples when model assumptions are satisfied. A variety of existing approaches for constructing such confidence intervals are summarized and the various methods are applied to an example to illustrate their use. A simulation study reveals that the newly proposed method yields the most accurate coverage probabilities under conditions more analogous to practice, where assumptions about normally distributed effect size estimates and known sampling variances only hold asymptotically.},
  publisher = {Wiley},
}

@Article{Wang-Zhang-2020,
  author = {Lijuan Wang and Qian Zhang},
  date = {2020-06},
  journaltitle = {Psychological Methods},
  title = {Investigating the impact of the time interval selection on autoregressive mediation modeling: Result interpretations, effect reporting, and temporal designs},
  doi = {10.1037/met0000235},
  issn = {1082-989X},
  number = {3},
  pages = {271--291},
  volume = {25},
  abstract = {This study investigates the impact of the time interval (the time passed between 2 consecutive measurements) selection on autoregressive mediation modeling (AMM). For a widely used autoregressive mediation model, via analytical derivations, we explained why and how the conventionally reported time-specific coefficient estimates (e.g., $\hat{a} \hat{bb}$ and $\hat{c}^{\prime}$ ) and inference results in AMM provide limited information and can arrive in even misleading conclusions about direct and indirect effects over time. Furthermore, under the stationarity assumption, we proposed an approach to calculate the overall direct and indirect effect estimates over time and the time lag lengths at which they reach maxima, using AMM results. The derivation results revealed that the overall direct and indirect effect curves are asymptotically invariant to the time interval selection, under stationarity. With finite samples and thus sampling errors and potential computing problems, however, our simulation results revealed that the overall indirect effect curves were better recovered when the time interval is selected to be closer to half of the time lag length at which the overall indirect effect reaches its maximum. An R function and an R Shiny app were developed to obtain the overall direct and indirect effect curves over time and facilitate the time interval selection using AMM results. Our findings provide another look at the connections between AMM and continuous time mediation modeling and the connections are discussed. },
  publisher = {American Psychological Association (APA)},
}

@Article{Wills-Resko-Ainette-etal-2004,
  author = {Thomas Ashby Wills and Jody A. Resko and Michael G. Ainette and Don Mendoza},
  date = {2004-06},
  journaltitle = {Psychology of Addictive Behaviors},
  title = {Role of parent support and peer support in adolescent substance use: A test of mediated effects},
  doi = {10.1037/0893-164x.18.2.122},
  issn = {0893-164X},
  number = {2},
  pages = {122--134},
  volume = {18},
  abstract = {This research tested comparative effects of parent and peer support on adolescent substance use (tobacco, alcohol, and marijuana) with data from 2 assessments of a multiethnic sample of 1,826 adolescents, mean age 12.3 years. Multiple regression analyses indicated that parental support was inversely related to substance use and that peer support was positively related to substance use, as a suppression effect. Structural modeling analyses indicated that effects of support were mediated through pathways involving good self-control, poor self-control, and risk-taking tendency; parent and peer support had different patterns of relations to these mediators. The mediators had pathways to substance use through positive and negative recent events and through peer affiliations. Effects for gender and ethnicity were also noted. Mechanisms of operation for parent and peer support are discussed.},
  publisher = {American Psychological Association (APA)},
}

@Article{Yang-2006,
  author = {Chih-Chien Yang},
  date = {2006-02},
  journaltitle = {Computational Statistics \& Data Analysis},
  title = {Evaluating latent class analysis models in qualitative phenotype identification},
  doi = {10.1016/j.csda.2004.11.004},
  issn = {0167-9473},
  number = {4},
  pages = {1090--1104},
  volume = {50},
  abstract = {The paper is aimed to investigate the performance of information criteria in selecting latent class analysis models which are often used in research of phenotype identification. Six information criteria and a sample size adjustment (Psychometrika 52 (1987) 333) are compared under various sample sizes and model dimensionalities. The simulation design is particularly meaningful for phenotypic research in practice. Results show that improvements by the sample size adjustment are considerable. In addition, the sample size and model dimensionality effects are found to be influential in the simulation study.},
  publisher = {Elsevier BV},
}

@Article{Yuan-Bentler-2000,
  author = {Ke-Hai Yuan and Peter M. Bentler},
  date = {2000-08},
  journaltitle = {Sociological Methodology},
  title = {Three likelihood-based methods for mean and covariance structure analysis with nonnormal missing data},
  doi = {10.1111/0081-1750.00078},
  number = {1},
  pages = {165--200},
  volume = {30},
  abstract = {Survey and longitudinal studies in the social and behavioral sciences generally contain missing data. Mean and covariance structure models play an important role in analyzing such data. Two promising methods for dealing with missing data are a direct maximum-likelihood and a two-stage approach based on the unstructured mean and covariance estimates obtained by the EM-algorithm. Typical assumptions under these two methods are ignorable nonresponse and normality of data. However, data sets in social and behavioral sciences are seldom normal, and experience with these procedures indicates that normal theory based methods for nonnormal data very often lead to incorrect model evaluations. By dropping the normal distribution assumption, we develop more accurate procedures for model inference. Based on the theory of generalized estimating equations, a way to obtain consistent standard errors of the two-stage estimates is given. The asymptotic efficiencies of different estimators are compared under various assumptions. We also propose a minimum chi-square approach and show that the estimator obtained by this approach is asymptotically at least as efficient as the two likelihood-based estimators for either normal or nonnormal data. The major contribution of this paper is that for each estimator, we give a test statistic whose asymptotic distribution is chisquare as long as the underlying sampling distribution enjoys finite fourth-order moments. We also give a characterization for each of the two likelihood ratio test statistics when the underlying distribution is nonnormal. Modifications to the likelihood ratio statistics are also given. Our working assumption is that the missing data mechanism is missing completely at random. Examples and Monte Carlo studies indicate that, for commonly encountered nonnormal distributions, the procedures developed in this paper are quite reliable even for samples with missing data that are missing at random.},
  publisher = {{SAGE} Publications},
}

@Article{Yuan-MacKinnon-2009,
  author = {Ying Yuan and David P. MacKinnon},
  date = {2009-12},
  journaltitle = {Psychological Methods},
  title = {Bayesian mediation analysis.},
  doi = {10.1037/a0016972},
  issn = {1082-989X},
  number = {4},
  pages = {301--322},
  volume = {14},
  abstract = {In this article, we propose Bayesian analysis of mediation effects. Compared with conventional frequentist mediation analysis, the Bayesian approach has several advantages. First, it allows researchers to incorporate prior information into the mediation analysis, thus potentially improving the efficiency of estimates. Second, under the Bayesian mediation analysis, inference is straightforward and exact, which makes it appealing for studies with small samples. Third, the Bayesian approach is conceptually simpler for multilevel mediation analysis. Simulation studies and analysis of 2 data sets are used to illustrate the proposed methods.},
  publisher = {American Psychological Association (APA)},
}

@Article{Zeileis-2004,
  author = {Achim Zeileis},
  date = {2004},
  journaltitle = {Journal of Statistical Software},
  title = {Econometric computing with {HC} and {HAC} covariance matrix estimators},
  doi = {10.18637/jss.v011.i10},
  number = {10},
  volume = {11},
  abstract = {Data described by econometric models typically contains autocorrelation and/or heteroskedasticity of unknown form and for inference in such models it is essential to use covariance matrix estimators that can consistently estimate the covariance of the model parameters. Hence, suitable heteroskedasticity consistent (HC) and heteroskedasticity and autocorrelation consistent (HAC) estimators have been receiving attention in the econometric literature over the last 20 years. To apply these estimators in practice, an implementation is needed that preferably translates the conceptual properties of the underlying theoretical frameworks into computational tools. In this paper, such an implementation in the package sandwich in the R system for statistical computing is described and it is shown how the suggested functions provide reusable components that build on readily existing functionality and how they can be integrated easily into new inferential procedures or applications. The toolbox contained in sandwich is extremely flexible and comprehensive, including specific functions for the most important HC and HAC estimators from the econometric literature. Several real-world data sets are used to illustrate how the functionality can be integrated into applications.},
  publisher = {Foundation for Open Access Statistic},
  annotation = {regression, regression-hc},
}

@Article{Zeileis-2006,
  author = {Achim Zeileis},
  date = {2006-08},
  journaltitle = {Journal of Statistical Software},
  title = {Object-oriented computation of sandwich estimators},
  doi = {10.18637/jss.v016.i09},
  number = {9},
  volume = {16},
  abstract = {Sandwich covariance matrix estimators are a popular tool in applied regression modeling for performing inference that is robust to certain types of model misspecification. Suitable implementations are available in the R system for statistical computing for certain model fitting functions only (in particular lm()), but not for other standard regression functions, such as glm(), nls(), or survreg(). Therefore, conceptual tools and their translation to computational tools in the package sandwich are discussed, enabling the computation of sandwich estimators in general parametric models. Object orientation can be achieved by providing a few extractor functions' most importantly for the empirical estimating functions' from which various types of sandwich estimators can be computed.},
  publisher = {Foundation for Open Access Statistic},
  annotation = {regression, regression-hc},
}
